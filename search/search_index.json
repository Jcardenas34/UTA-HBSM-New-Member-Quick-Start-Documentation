{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Charged Higgs UTA Quick Start Documentation The way that this documentation is structured is in a series of links. Very many resources That I used to learn the practicle skills needed to do Particle Physics were from random internet links and not books. So very often the resource that I found most helpful, wether it be a reference document, a video series on youtube, or a tutorial outlining steps will look like this. Take the first step... It's a link to cool LO-FI music that you can listen to Take what you can from these links, and add new ones to this document if you feel youve learned more from it. Meetings Our general meetings that take place every Tuesday and Friday in CPB 128 on the UTA campus, we also have a page where we post our results for the week, that also hosts a video calling software called \"vidyo\" that we use when we are unable to meet physically here . This is hosted on a CERN meeting page called \"Indico\", that hosts a video room through the Vidyo software and allows us to have a place to post our results or weekly update powerpoints. Unfortunately you are only able to access the chatroom if you have a CERN account, and it requires a pass pin to get into the chatroom. Ask us for the password to the page, as it is unwise to post it in a public place like this. Mattermost Collaboration area We as a group, as well as a lot of other CERN affiliated groups have a place where we can all collaborate and message each other in case we need some help or need to post results, before an official meeting. The software is called the Mattermost, and unfortunately, you also need a CERN account to access it. Note that this isnt just for our group, there are hundreds of mattermost channels that you can search for and join in case you are interested in certain topics. Anything from a ROOT matter most to a machine learning mattermost, and many more are available. Works in progress All of it but specifically id like to add More about ATLAS and our charged higgs process, super symmetry and all that the ATLAS detector how to write a paper using overleaf or Latex Where to find papers Getting started with a CERN account Want to make a page like this one? It was made using a software called mkdocs, and the full documentation is here mkdocs.org .","title":"Home"},{"location":"#charged-higgs-uta","text":"","title":"Charged Higgs UTA"},{"location":"#quick-start-documentation","text":"The way that this documentation is structured is in a series of links. Very many resources That I used to learn the practicle skills needed to do Particle Physics were from random internet links and not books. So very often the resource that I found most helpful, wether it be a reference document, a video series on youtube, or a tutorial outlining steps will look like this. Take the first step... It's a link to cool LO-FI music that you can listen to Take what you can from these links, and add new ones to this document if you feel youve learned more from it.","title":"Quick Start Documentation"},{"location":"#meetings","text":"Our general meetings that take place every Tuesday and Friday in CPB 128 on the UTA campus, we also have a page where we post our results for the week, that also hosts a video calling software called \"vidyo\" that we use when we are unable to meet physically here . This is hosted on a CERN meeting page called \"Indico\", that hosts a video room through the Vidyo software and allows us to have a place to post our results or weekly update powerpoints. Unfortunately you are only able to access the chatroom if you have a CERN account, and it requires a pass pin to get into the chatroom. Ask us for the password to the page, as it is unwise to post it in a public place like this.","title":"Meetings"},{"location":"#mattermost-collaboration-area","text":"We as a group, as well as a lot of other CERN affiliated groups have a place where we can all collaborate and message each other in case we need some help or need to post results, before an official meeting. The software is called the Mattermost, and unfortunately, you also need a CERN account to access it. Note that this isnt just for our group, there are hundreds of mattermost channels that you can search for and join in case you are interested in certain topics. Anything from a ROOT matter most to a machine learning mattermost, and many more are available.","title":"Mattermost Collaboration area"},{"location":"#works-in-progress","text":"All of it but specifically id like to add More about ATLAS and our charged higgs process, super symmetry and all that the ATLAS detector how to write a paper using overleaf or Latex Where to find papers Getting started with a CERN account Want to make a page like this one? It was made using a software called mkdocs, and the full documentation is here mkdocs.org .","title":"Works in progress"},{"location":"The_LHC/","text":"The LHC (Large Hadron Collider) We get our data from the ATLAS detector at the Large hadron Collider in Geneva Switzerland. The LHC is a proton collider that is 27km (16 miles) in circumference and has components ranging from 50 to 175 meters underground. We put or collider underground so that particles coming from outer space get caught in the earth above it, and don\u2019t interfere with our experiments. It accelerates bunches of protons to a center of mass energy of 13TeV and collides them at 4 points along the ring where the detectors are located, every 25 Nano-seconds. The detectors are called LHCb, ATLAS, ALICE and CMS. Once the protons are collided, they create a shower of other particles that are then deposited into the walls of our detector, where they can then be interpreted as data for us to work with. Now its important to note that most of the particles created in the collision have a very short lifetime, and decay before they reach the walls of the detector. They decay into more stable, Final state particles, that we can then use to reconstruct what created them. The ATLAS Detector","title":"The Detectors"},{"location":"The_LHC/#the-lhc-large-hadron-collider","text":"We get our data from the ATLAS detector at the Large hadron Collider in Geneva Switzerland. The LHC is a proton collider that is 27km (16 miles) in circumference and has components ranging from 50 to 175 meters underground. We put or collider underground so that particles coming from outer space get caught in the earth above it, and don\u2019t interfere with our experiments. It accelerates bunches of protons to a center of mass energy of 13TeV and collides them at 4 points along the ring where the detectors are located, every 25 Nano-seconds. The detectors are called LHCb, ATLAS, ALICE and CMS. Once the protons are collided, they create a shower of other particles that are then deposited into the walls of our detector, where they can then be interpreted as data for us to work with. Now its important to note that most of the particles created in the collision have a very short lifetime, and decay before they reach the walls of the detector. They decay into more stable, Final state particles, that we can then use to reconstruct what created them.","title":"The LHC (Large Hadron Collider)"},{"location":"The_LHC/#the-atlas-detector","text":"","title":"The ATLAS Detector"},{"location":"about/","text":"HBSM Group at UTA The Need for More This documentation was created out of the necessity to get new undergraduate as well as new Graduate students ready to do research as fast as possible. At the start There was very little if any documentation on how to use critical software necessary to become a particle physicist. Such as how to use ROOT, where to find papers, where to go to learn to code in C++ and python or even how to use the Tier3 cluster here at UTA much less about what it is or where its located. It is my idea that if I can provide a central place where all of this information is located, I can expidite the progress of this group, as well as the HEP (High Energy Physics) Community here at UTA. People will come and go at UTA, but here are the current students and faculty you will become aquainted with. As a User to this documentation, I ask you, please help contribute to this document so that UTA's Physics Department can grow stonger, closer, and more competative in the coming years, by helping students who are learning, just like you, understand how to become a physicist. This documentation was made with MkDocs","title":"About"},{"location":"about/#hbsm-group-at-uta","text":"","title":"HBSM Group at UTA"},{"location":"about/#the-need-for-more","text":"This documentation was created out of the necessity to get new undergraduate as well as new Graduate students ready to do research as fast as possible. At the start There was very little if any documentation on how to use critical software necessary to become a particle physicist. Such as how to use ROOT, where to find papers, where to go to learn to code in C++ and python or even how to use the Tier3 cluster here at UTA much less about what it is or where its located. It is my idea that if I can provide a central place where all of this information is located, I can expidite the progress of this group, as well as the HEP (High Energy Physics) Community here at UTA. People will come and go at UTA, but here are the current students and faculty you will become aquainted with. As a User to this documentation, I ask you, please help contribute to this document so that UTA's Physics Department can grow stonger, closer, and more competative in the coming years, by helping students who are learning, just like you, understand how to become a physicist. This documentation was made with MkDocs","title":"The Need for More"},{"location":"coding/","text":"Coding Languages and the Command Line Everyone starts out with a diffent level of experience in coding, If you take me for example I came into the Graduate program here knowing little to nothing about how to code, just basic for loops and varible declarations. But in joining the High Energy Community you will no doubt have to become a master In 2 programming languages they are C++ and Python . Where I have linked the sources to thier documentation in the links there. But before you can embark on your jouney in learning these programming languaes, you must learn How to navigate your command line terminal This is what will let you take your first steps so to say, It is what will allow you to run the C++ and Python scripts that you write, as well as copy files to and from other computers. After you feel like you can do the very basic commands out of muscle memory, such as creating files and directories, moving and copying those files you can start learning about the coding languages we use on a daily basis, Python and C++.","title":"Command Line"},{"location":"coding/#coding-languages-and-the-command-line","text":"Everyone starts out with a diffent level of experience in coding, If you take me for example I came into the Graduate program here knowing little to nothing about how to code, just basic for loops and varible declarations. But in joining the High Energy Community you will no doubt have to become a master In 2 programming languages they are C++ and Python . Where I have linked the sources to thier documentation in the links there. But before you can embark on your jouney in learning these programming languaes, you must learn How to navigate your command line terminal This is what will let you take your first steps so to say, It is what will allow you to run the C++ and Python scripts that you write, as well as copy files to and from other computers. After you feel like you can do the very basic commands out of muscle memory, such as creating files and directories, moving and copying those files you can start learning about the coding languages we use on a daily basis, Python and C++.","title":"Coding Languages and the Command Line"},{"location":"cyberduck/","text":"Accelerating your work flow When I first started out, I was making all of my file edits through the emacs command in the terminal. And although emacs is a very powerful text editor, it can get a little bit difficult to work with when youre using it remotely, It does not support any in window mouse clicks and it occupies the entire terminal window while youre working, making you unable to view the files and directory name that youre working in. and maybe you want to use a text editor that has a lot more features, and not be tied down to using emacs, although you will definitly have to use emacs for quicker edits. It turns out there is a way that you can edit files on a remote server while using the text editor of your choice, through an sftp software called Cyberduck","title":"Cyberduck and Sublime"},{"location":"cyberduck/#accelerating-your-work-flow","text":"When I first started out, I was making all of my file edits through the emacs command in the terminal. And although emacs is a very powerful text editor, it can get a little bit difficult to work with when youre using it remotely, It does not support any in window mouse clicks and it occupies the entire terminal window while youre working, making you unable to view the files and directory name that youre working in. and maybe you want to use a text editor that has a lot more features, and not be tied down to using emacs, although you will definitly have to use emacs for quicker edits. It turns out there is a way that you can edit files on a remote server while using the text editor of your choice, through an sftp software called Cyberduck","title":"Accelerating your work flow"},{"location":"github/","text":"Getting Started with Github Git hub, is a place for you to store your code on a free online cloud server, as well as provides a way for multiple people to work on the same piece of code simultaneously, similar to a google doc if youve ever used one. Lots of groups in ATLAS use Github because they have lots of members that need to work towards completing a single large project. Lets say that there is a big project that needs to be worked on, such as creating a software that will use neural networks to distinguish between physics objects and plot thier performance. Here we can see 2 major tasks, we need someone to design these neural networks and another to create the software to plot how well they are doing. It would be very difficuly for person or a single team of people to work on both aspects, given that they are both sufficiently complicated. So it would be useful to separate the project into pieces, so that one group works on creating and optimizing the neural networks and the other focuses on creating the software to plot the performance of these networks once they are up and running. In the end though, all components of the project have to be merged together to give you a final working product that does calculation and produces plots all at once. This is where github becomes useful, as it provides the ability for multiple users to copy the initial master branch of code, make thier changes and contributions, and then push it back to the \"master branch\" to be reviewed by team mates and finally merged with the final product. Each person in a team would create a copy of the code they will work on (called a fork) and put it on thier computer. They would then work on thier dedicated aspect locally and when significant changes have been made, they \"push\" thier code back to the github website for review by thier teammates. If thier teammates like the changes/ contributions, the team lead can choose to merge the contribution to the master project called the master branch. Backups Another great aspect of Git hub is that it can work as a place to store a backup of your code, so that if a project that you are working on is stored on a cluster that goes down (becomes unusable for a period of time), which they very often do, you still have access to your code and can copy or (git clone) your work to another computer or cluster to continue working. I cannot tell you how many times either the tier3 or GPU cluster went down and I was forced to rewrite major pieces of my code or wait until the cluster went back online, which sometimes can be weeks, depending on what caused the issue. So thats it for an introduction, I think this video series is pretty good at explaining what Git and Github is and how to use it. Learn Git/Github here GitLab There also exists another more comprehensive version of GitHub called GitLab, that is used to larger collaborations and besides having a cooler logo, it has expanded functionality that comes in handy when working with larger groups of people. ATLAS and other collaborations in the LCH use GitLab instead of Git hub because of its features, while we as a smaller group at UTA would more than likely use a github page to store all our code and collaborate, it is important to know of the existance of GitLab for when you eventually have to use it as part of a collaboration wide project.","title":"Github"},{"location":"github/#getting-started-with-github","text":"Git hub, is a place for you to store your code on a free online cloud server, as well as provides a way for multiple people to work on the same piece of code simultaneously, similar to a google doc if youve ever used one. Lots of groups in ATLAS use Github because they have lots of members that need to work towards completing a single large project. Lets say that there is a big project that needs to be worked on, such as creating a software that will use neural networks to distinguish between physics objects and plot thier performance. Here we can see 2 major tasks, we need someone to design these neural networks and another to create the software to plot how well they are doing. It would be very difficuly for person or a single team of people to work on both aspects, given that they are both sufficiently complicated. So it would be useful to separate the project into pieces, so that one group works on creating and optimizing the neural networks and the other focuses on creating the software to plot the performance of these networks once they are up and running. In the end though, all components of the project have to be merged together to give you a final working product that does calculation and produces plots all at once. This is where github becomes useful, as it provides the ability for multiple users to copy the initial master branch of code, make thier changes and contributions, and then push it back to the \"master branch\" to be reviewed by team mates and finally merged with the final product. Each person in a team would create a copy of the code they will work on (called a fork) and put it on thier computer. They would then work on thier dedicated aspect locally and when significant changes have been made, they \"push\" thier code back to the github website for review by thier teammates. If thier teammates like the changes/ contributions, the team lead can choose to merge the contribution to the master project called the master branch.","title":"Getting Started with Github"},{"location":"github/#backups","text":"Another great aspect of Git hub is that it can work as a place to store a backup of your code, so that if a project that you are working on is stored on a cluster that goes down (becomes unusable for a period of time), which they very often do, you still have access to your code and can copy or (git clone) your work to another computer or cluster to continue working. I cannot tell you how many times either the tier3 or GPU cluster went down and I was forced to rewrite major pieces of my code or wait until the cluster went back online, which sometimes can be weeks, depending on what caused the issue. So thats it for an introduction, I think this video series is pretty good at explaining what Git and Github is and how to use it. Learn Git/Github here","title":"Backups"},{"location":"github/#gitlab","text":"There also exists another more comprehensive version of GitHub called GitLab, that is used to larger collaborations and besides having a cooler logo, it has expanded functionality that comes in handy when working with larger groups of people. ATLAS and other collaborations in the LCH use GitLab instead of Git hub because of its features, while we as a smaller group at UTA would more than likely use a github page to store all our code and collaborate, it is important to know of the existance of GitLab for when you eventually have to use it as part of a collaboration wide project.","title":"GitLab"},{"location":"neuralnets/","text":"Getting Started with Neural Networks As of today, the field of particle physics is moving towards the use of Neural Networks to help us as particle physicists do our job. They are quickly showing they can out perform the previous data analysis tool called the Boosted Decision Tree (BDT), When It comes to distinguishing a process that we are looking for from something that we arent, since they ofentimes look similar and can be confused. The resource that I suggest using to learn about neural networks is the series by 3 blue 1 brown, here But what is a Neural Network? | Deep learning, chapter 1 He has created a somewhat easy to follow series of videos that give you a basic idea of how a neural networks functions, as well as gets you familiar with some of the jargon. At the time of this writing, there are 4 videos, so make sure to watch them all. They will serve as a sort of primer for the second document below, or vice versa. Another useful document is here Neural Networks From Scratch This was a resource suggested by Susan Bataju, a member of our group. Its a tutorial series that teaches you about 3 of the most common types of neural networks, the Deep, Recurrant and Convolutional Neural networks, (DNN,RNN and CNN. Its a pretty detailed set of tutorials that even gives some examples in code of how to build a neural net.","title":"Neural Networks"},{"location":"neuralnets/#getting-started-with-neural-networks","text":"As of today, the field of particle physics is moving towards the use of Neural Networks to help us as particle physicists do our job. They are quickly showing they can out perform the previous data analysis tool called the Boosted Decision Tree (BDT), When It comes to distinguishing a process that we are looking for from something that we arent, since they ofentimes look similar and can be confused. The resource that I suggest using to learn about neural networks is the series by 3 blue 1 brown, here But what is a Neural Network? | Deep learning, chapter 1 He has created a somewhat easy to follow series of videos that give you a basic idea of how a neural networks functions, as well as gets you familiar with some of the jargon. At the time of this writing, there are 4 videos, so make sure to watch them all. They will serve as a sort of primer for the second document below, or vice versa. Another useful document is here Neural Networks From Scratch This was a resource suggested by Susan Bataju, a member of our group. Its a tutorial series that teaches you about 3 of the most common types of neural networks, the Deep, Recurrant and Convolutional Neural networks, (DNN,RNN and CNN. Its a pretty detailed set of tutorials that even gives some examples in code of how to build a neural net.","title":"Getting Started with Neural Networks"},{"location":"python/","text":"Python and C++ our Main Programming Languages There are certian programming languages that people use for differnt project types, for example, if you were to try and build a website you might want to learn to code in HTML or if you were in Data science it might be useful to learn the R programming language. It just so happens that in Particle physics, the 2 programming languages that are most useful to us are C++ and Python. C++ for its speed in large projects and compatability with ROOT (Which youll learn about later), and Python, for its ease of use, compatability with common machine learning software such as Keras and Tensorflow as well as its increaced simplicity when using ROOT. Like I said previously, Its very useful to get to a working proficiency with both languages but our group has recently trasitioned to working with Python, as it is easier to use with ROOT, and for our Neural Network projects. So where can you learn how to code in these languages? Some of the youtube series that helped me get familiar with Python are here and C++ here Dont get me wrong, these series of videos are VERY long, and will take a while to get through them, but by the end you will have a better understanding of how to write code in these languages. And please, dont feel like you have to watch these videos sequentially from start to finish, skip around and watch the ones you have questions about.","title":"Python/C++"},{"location":"python/#python-and-c-our-main-programming-languages","text":"There are certian programming languages that people use for differnt project types, for example, if you were to try and build a website you might want to learn to code in HTML or if you were in Data science it might be useful to learn the R programming language. It just so happens that in Particle physics, the 2 programming languages that are most useful to us are C++ and Python. C++ for its speed in large projects and compatability with ROOT (Which youll learn about later), and Python, for its ease of use, compatability with common machine learning software such as Keras and Tensorflow as well as its increaced simplicity when using ROOT. Like I said previously, Its very useful to get to a working proficiency with both languages but our group has recently trasitioned to working with Python, as it is easier to use with ROOT, and for our Neural Network projects. So where can you learn how to code in these languages? Some of the youtube series that helped me get familiar with Python are here and C++ here Dont get me wrong, these series of videos are VERY long, and will take a while to get through them, but by the end you will have a better understanding of how to write code in these languages. And please, dont feel like you have to watch these videos sequentially from start to finish, skip around and watch the ones you have questions about.","title":"Python and C++ our Main Programming Languages"},{"location":"root/","text":"Everything ROOT As you have probably already realized, Doing physics has a lot of prerequisites. You have to learn a LOT of stuff before you can start doing actual research. But this tool, ROOT is the primary one that we use in the High energy physics communuty. It is a programming software that's compatible with both C++ and Python and used to Analyze data and create the plots that we use to visualize our results. It has a lot of capabilities, and using it with python makes the whole process of data analysis just a bit easier, Although ROOT is notoriously complicated and difficult to learn, My hope is to try and give you the basics so you can get on your way to performing data analysis and contribute to group progress as quickly as possible.","title":"ROOT"},{"location":"root/#everything-root","text":"As you have probably already realized, Doing physics has a lot of prerequisites. You have to learn a LOT of stuff before you can start doing actual research. But this tool, ROOT is the primary one that we use in the High energy physics communuty. It is a programming software that's compatible with both C++ and Python and used to Analyze data and create the plots that we use to visualize our results. It has a lot of capabilities, and using it with python makes the whole process of data analysis just a bit easier, Although ROOT is notoriously complicated and difficult to learn, My hope is to try and give you the basics so you can get on your way to performing data analysis and contribute to group progress as quickly as possible.","title":"Everything ROOT"},{"location":"supercomputer/","text":"The GPU Cluster There exists another cluster on campus that is different from the tier3. It is a GPU super computer managed by Dr.Farbin here at UTA. This cluster is similar to the tier3, but instead of having compute nodes that have many CPUs the nodes on the super computer host multiple GPUs (Graphical Processing units) that are capable of executing many tasks in parallel. The general purpose for a GPU cluster is to do work in training machine learning algorithms such as the ones found in Neural networks. With Dr. Farbin's permission, and if you begin to do work with neural networks, you will find this cluster to be extremely useful. You will need to email Dr.Farbin at afarbin@uta.edu for him to give you an account on the supercomputer, but once you have all the login information you should be able to access the computer by ssh-ing into it with the command below ssh -X <user_name>@orodruin.uta.edu The Head node The head node, just like the head node on the tier3, is where you will be doing most of your work. Similar to ther tier3, the head node on the super computer does have some GPU resources, it has 4 GeForce GTX 1080 GPUs that can be used to test short scripts, although it is discouraged to run very large and computing intensive pieces of code on it, since it will interfere with others trying to login in or test thier scripts. It is best practice to submit your code via the the PBS batch schedueler like on the tier3 or log into the compute node directly and submit your code ro be run. GPU compute nodes The super computer has 4 GPU compute nodes and 1 ordinary CPU node, which can be accessed once you have logged into the main node as indicated above. Thier names are The Count, which has 10 TITAN X (Pascal) GPUs for use ssh thecount Thing One, has 4 GeForce GTX 1080 GPUs and together are just as powerful as the 10 GPUs on the count. ssh thingone Thing Two, which also has 4 GeForce GTX 1080 GPUs ssh thingtwo","title":"The Super Computer"},{"location":"supercomputer/#the-gpu-cluster","text":"There exists another cluster on campus that is different from the tier3. It is a GPU super computer managed by Dr.Farbin here at UTA. This cluster is similar to the tier3, but instead of having compute nodes that have many CPUs the nodes on the super computer host multiple GPUs (Graphical Processing units) that are capable of executing many tasks in parallel. The general purpose for a GPU cluster is to do work in training machine learning algorithms such as the ones found in Neural networks. With Dr. Farbin's permission, and if you begin to do work with neural networks, you will find this cluster to be extremely useful. You will need to email Dr.Farbin at afarbin@uta.edu for him to give you an account on the supercomputer, but once you have all the login information you should be able to access the computer by ssh-ing into it with the command below ssh -X <user_name>@orodruin.uta.edu","title":"The GPU Cluster"},{"location":"supercomputer/#the-head-node","text":"The head node, just like the head node on the tier3, is where you will be doing most of your work. Similar to ther tier3, the head node on the super computer does have some GPU resources, it has 4 GeForce GTX 1080 GPUs that can be used to test short scripts, although it is discouraged to run very large and computing intensive pieces of code on it, since it will interfere with others trying to login in or test thier scripts. It is best practice to submit your code via the the PBS batch schedueler like on the tier3 or log into the compute node directly and submit your code ro be run.","title":"The Head node"},{"location":"supercomputer/#gpu-compute-nodes","text":"The super computer has 4 GPU compute nodes and 1 ordinary CPU node, which can be accessed once you have logged into the main node as indicated above. Thier names are The Count, which has 10 TITAN X (Pascal) GPUs for use ssh thecount Thing One, has 4 GeForce GTX 1080 GPUs and together are just as powerful as the 10 GPUs on the count. ssh thingone Thing Two, which also has 4 GeForce GTX 1080 GPUs ssh thingtwo","title":"GPU compute nodes"},{"location":"tier3/","text":"The Tier3: The High Energy Physics (HEP) Cluster If you stay in the computing world it is more than likely youll run into a situation where youll have to use a computer cluster. These computer clusters are used when you have a piece of code that requires a lot of computing resources such as the need to run on multiple processors or requires much more RAM than one computer can provide. A lot of the time in High Energy Physics we create such pieces of code. The computing cluster that we have access to in the High Energy Physics (HEP) department is called the Tier3. It is part of 3 clusters associated with the LHC (Tier1, tier2, and Tier3), and certian universities host them, we are one of them. The tier3 is what does all the computation for analysis, the others have other dedicated processes. There are other computing clusters that CERN uses, such as the GRID and lxplus, but from day to day since the tier3 is located on campus, you will be using this most of the time. In general the way that clusters are structured is as such, The Head node This is the computer that you or any person who wants to access the cluster logs into via \"ssh\". If you have not logged into the tier3 yet, you will have to contact the cluster manager at UTA, at the time of writing this is Mark Sosebee: sosebee@exchange.uta.edu who will create an account for you and give you instructions on how to log in. Here is the basic command that you will use at your command line to login to the tier3 ssh -X <user_name>@master.tier3-ATLAS.uta.edu This Head node, is where you will be doing most of your work, and where you will submit your code to what we call a batch scheduler that will effectively run your code. The Batch schedueler is the program that distributes your job to the rest of the computers connected to the head node, called the compute nodes and tells them to run the script you are writing, there are many types of bactch scheduelers that different clusters use, some popular ones are \"Slurm\", named after the famous drink in futurama, Condor which is used by the lxplus cluster that CERN uses and PBS (Portable Batch Schedueler) which we use on the tier3. Each one of these batch scheduelers has a set of commands you can type to give you information on the current jobs that are runnning on the cluster as well as commands that will submit your code to the cluster such as \"qsub\". BUT, In order to make the compute nodes process your code, instead of typing out \"qsub script_you_want_to_run.suffix\" where suffix can be anything from .py to .cxx, you must create what we call a submission script. This is a script with a siffix of \".sub\" that lets the batch schedueler know how many processors you want to use, as well as how much RAM and compute time you need. and also contains all of the commands that you want the compute nodes to execute. There will be an example of how to write a basic submission script as well as links to the documentation on all the possible options in a section below. As of now, the tier-3 cluster is contained in one rack in CPB 115. This is the machine room where the tier-2 system we operate for ATLAS is located. By comparison the tier-2 system occupies 27 racks. The Compute nodes The compute nodes are what do all of the computation in your script, and they are all connected to the head node via the batch schedueler, which as I said above determines via a submission script how many compute nodes will process your code and many other computing resources are needed. The Batch Script Reference Documents There exists user documentation and a reference guide on how to use the pbs batch system, linked below User Guide for PBS Reference guide for PBS The user guide is a more comprehensive document that teaches you about most of the aspects of the PBS (Portable batch system), the reference guide is more of a summary of all possible commands that you can input, both in the PBS script and in the command prompt How to make a Tier3 not timeout So at the time of writing, the tier3 has an annoying feature where it disconnects your login session in the terminal after a few minutes of inactivity. This is particularly annoying when you have to ssh into multiple clusters, and must leave the tier3 terminal inactive for a while, but will return to it later. When you return, you see that the termial is now inactive, and you must log in again, and navigate to where you were previously. Luckily, there are a few changes you can make in your ssh_config script that will help you extend the time before timeout. These instructions work on a mac , but if you have some sort of linux based command line terminal installed on your computer, is should work just the same. For windows you can either use \"commander\" or \"git terminal\" or \"ubuntu bash shell\", I will add more on how to install this in the Accelerating your workflow part of the page later. When you first open your terminal, you will be logged into the home directory of your personal computer. You will want to open this file with whatever text editor you prefer. /etc/ssh/ssh_config by typing, either, emacs, sublime, nano, vim, atom or your preferred text editor name, and then the line above. This is a file that tells the command prompt how to insteract with any computer that you choose to ssh into, weather that be the tier3 or another cluster somewhere else. All of the lines in this file should be commented out, except for a small section near the bottom of the page. which should look like this. Host * SendEnv LANG LC_* you want to add a few lines to it so that It looks like this ``` Host * SendEnv LANG LC_* ServerAliveInterval 120 ServerAliveCountMax 720 ``` and save the changes. The firstaddition we made makes it so a blank message is sent to the server every 120 seconds in order to keep the connection, the second line we added makes it so that it will log you off if this is done 720 times, so 720 x 120 seconds = 24 hours, you can adjust this second number to 30 for one hour tuntil timeout, or to 60 for 2 hours of sustained login during 2 hours of inactivity.","title":"The Tier 3"},{"location":"tier3/#the-tier3-the-high-energy-physics-hep-cluster","text":"If you stay in the computing world it is more than likely youll run into a situation where youll have to use a computer cluster. These computer clusters are used when you have a piece of code that requires a lot of computing resources such as the need to run on multiple processors or requires much more RAM than one computer can provide. A lot of the time in High Energy Physics we create such pieces of code. The computing cluster that we have access to in the High Energy Physics (HEP) department is called the Tier3. It is part of 3 clusters associated with the LHC (Tier1, tier2, and Tier3), and certian universities host them, we are one of them. The tier3 is what does all the computation for analysis, the others have other dedicated processes. There are other computing clusters that CERN uses, such as the GRID and lxplus, but from day to day since the tier3 is located on campus, you will be using this most of the time. In general the way that clusters are structured is as such,","title":"The Tier3: The High Energy Physics (HEP) Cluster"},{"location":"tier3/#the-head-node","text":"This is the computer that you or any person who wants to access the cluster logs into via \"ssh\". If you have not logged into the tier3 yet, you will have to contact the cluster manager at UTA, at the time of writing this is Mark Sosebee: sosebee@exchange.uta.edu who will create an account for you and give you instructions on how to log in. Here is the basic command that you will use at your command line to login to the tier3 ssh -X <user_name>@master.tier3-ATLAS.uta.edu This Head node, is where you will be doing most of your work, and where you will submit your code to what we call a batch scheduler that will effectively run your code. The Batch schedueler is the program that distributes your job to the rest of the computers connected to the head node, called the compute nodes and tells them to run the script you are writing, there are many types of bactch scheduelers that different clusters use, some popular ones are \"Slurm\", named after the famous drink in futurama, Condor which is used by the lxplus cluster that CERN uses and PBS (Portable Batch Schedueler) which we use on the tier3. Each one of these batch scheduelers has a set of commands you can type to give you information on the current jobs that are runnning on the cluster as well as commands that will submit your code to the cluster such as \"qsub\". BUT, In order to make the compute nodes process your code, instead of typing out \"qsub script_you_want_to_run.suffix\" where suffix can be anything from .py to .cxx, you must create what we call a submission script. This is a script with a siffix of \".sub\" that lets the batch schedueler know how many processors you want to use, as well as how much RAM and compute time you need. and also contains all of the commands that you want the compute nodes to execute. There will be an example of how to write a basic submission script as well as links to the documentation on all the possible options in a section below. As of now, the tier-3 cluster is contained in one rack in CPB 115. This is the machine room where the tier-2 system we operate for ATLAS is located. By comparison the tier-2 system occupies 27 racks.","title":"The Head node"},{"location":"tier3/#the-compute-nodes","text":"The compute nodes are what do all of the computation in your script, and they are all connected to the head node via the batch schedueler, which as I said above determines via a submission script how many compute nodes will process your code and many other computing resources are needed.","title":"The Compute nodes"},{"location":"tier3/#the-batch-script","text":"","title":"The Batch Script"},{"location":"tier3/#reference-documents","text":"There exists user documentation and a reference guide on how to use the pbs batch system, linked below User Guide for PBS Reference guide for PBS The user guide is a more comprehensive document that teaches you about most of the aspects of the PBS (Portable batch system), the reference guide is more of a summary of all possible commands that you can input, both in the PBS script and in the command prompt","title":"Reference Documents"},{"location":"tier3/#how-to-make-a-tier3-not-timeout","text":"So at the time of writing, the tier3 has an annoying feature where it disconnects your login session in the terminal after a few minutes of inactivity. This is particularly annoying when you have to ssh into multiple clusters, and must leave the tier3 terminal inactive for a while, but will return to it later. When you return, you see that the termial is now inactive, and you must log in again, and navigate to where you were previously. Luckily, there are a few changes you can make in your ssh_config script that will help you extend the time before timeout. These instructions work on a mac , but if you have some sort of linux based command line terminal installed on your computer, is should work just the same. For windows you can either use \"commander\" or \"git terminal\" or \"ubuntu bash shell\", I will add more on how to install this in the Accelerating your workflow part of the page later. When you first open your terminal, you will be logged into the home directory of your personal computer. You will want to open this file with whatever text editor you prefer. /etc/ssh/ssh_config by typing, either, emacs, sublime, nano, vim, atom or your preferred text editor name, and then the line above. This is a file that tells the command prompt how to insteract with any computer that you choose to ssh into, weather that be the tier3 or another cluster somewhere else. All of the lines in this file should be commented out, except for a small section near the bottom of the page. which should look like this. Host * SendEnv LANG LC_* you want to add a few lines to it so that It looks like this ``` Host * SendEnv LANG LC_* ServerAliveInterval 120 ServerAliveCountMax 720 ``` and save the changes. The firstaddition we made makes it so a blank message is sent to the server every 120 seconds in order to keep the connection, the second line we added makes it so that it will log you off if this is done 720 times, so 720 x 120 seconds = 24 hours, you can adjust this second number to 30 for one hour tuntil timeout, or to 60 for 2 hours of sustained login during 2 hours of inactivity.","title":"How to make a Tier3 not timeout"}]}