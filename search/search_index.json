{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Charged Higgs UTA Quick Start Documentation The way that this documentation is structured is in a series of links. Very many resources That I used to learn the practicle skills needed to do Particle Physics were from random internet links and not books. So very often the resource that I found most helpful, wether it be a reference document, a video series on youtube, or a tutorial outlining steps will look like this. Take the first step... It's a link to cool LO-FI music that you can listen to Take what you can from these links, and add new ones to this document if you feel youve learned more from it. Our general meetings that take place every tuesday and Friday have a page where we post our results for the week as well as a link to the video calling software that we use when we are unable to meet physically here ask us for the password to the page, as it is unwise to post it in a public place like this. For full documentation visit mkdocs.org . Works in progress All of it but specifically id like to add More about ATLAS and our charged higgs process, super symmetry and all that the ATLAS detector how to write a paper using overleaf or Latex Where to find papers group matter most Getting started with a CERN account Vidyo Common practices tools and resources people in the collaboration use like vidyo and indico","title":"Home"},{"location":"#charged-higgs-uta","text":"","title":"Charged Higgs UTA"},{"location":"#quick-start-documentation","text":"The way that this documentation is structured is in a series of links. Very many resources That I used to learn the practicle skills needed to do Particle Physics were from random internet links and not books. So very often the resource that I found most helpful, wether it be a reference document, a video series on youtube, or a tutorial outlining steps will look like this. Take the first step... It's a link to cool LO-FI music that you can listen to Take what you can from these links, and add new ones to this document if you feel youve learned more from it. Our general meetings that take place every tuesday and Friday have a page where we post our results for the week as well as a link to the video calling software that we use when we are unable to meet physically here ask us for the password to the page, as it is unwise to post it in a public place like this. For full documentation visit mkdocs.org .","title":"Quick Start Documentation"},{"location":"#works-in-progress","text":"All of it but specifically id like to add More about ATLAS and our charged higgs process, super symmetry and all that the ATLAS detector how to write a paper using overleaf or Latex Where to find papers group matter most Getting started with a CERN account Vidyo Common practices tools and resources people in the collaboration use like vidyo and indico","title":"Works in progress"},{"location":"The_LHC/","text":"The LHC (Large Hadron Collider) We get our data from the ATLAS detector at the Large hadron Collider in Geneva Switzerland. The LHC is a proton collider that is 27km (16 miles) in circumference and has components ranging from 50 to 175 meters underground. We put or collider underground so that particles coming from outer space get caught in the earth above it, and don\u2019t interfere with our experiments. It accelerates bunches of protons to a center of mass energy of 13TeV and collides them at 4 points along the ring where the detectors are located, every 25 Nano-seconds. The detectors are called LHCb, ATLAS, ALICE and CMS. Once the protons are collided, they create a shower of other particles that are then deposited into the walls of our detector, where they can then be interpreted as data for us to work with. Now its important to note that most of the particles created in the collision have a very short lifetime, and decay before they reach the walls of the detector. They decay into more stable, Final state particles, that we can then use to reconstruct what created them.","title":"The LHC"},{"location":"The_LHC/#the-lhc-large-hadron-collider","text":"We get our data from the ATLAS detector at the Large hadron Collider in Geneva Switzerland. The LHC is a proton collider that is 27km (16 miles) in circumference and has components ranging from 50 to 175 meters underground. We put or collider underground so that particles coming from outer space get caught in the earth above it, and don\u2019t interfere with our experiments. It accelerates bunches of protons to a center of mass energy of 13TeV and collides them at 4 points along the ring where the detectors are located, every 25 Nano-seconds. The detectors are called LHCb, ATLAS, ALICE and CMS. Once the protons are collided, they create a shower of other particles that are then deposited into the walls of our detector, where they can then be interpreted as data for us to work with. Now its important to note that most of the particles created in the collision have a very short lifetime, and decay before they reach the walls of the detector. They decay into more stable, Final state particles, that we can then use to reconstruct what created them.","title":"The LHC (Large Hadron Collider)"},{"location":"about/","text":"HBSM Group at UTA The Need for More This documentation was created out of the necessity to get new undergraduate as well as new Graduate students ready to do research as fast as possible. At the start There was very little if any documentation on how to use critical software necessary to become a particle physicist. Such as how to use ROOT, where to find papers, where to go to learn to code in C++ and python or even how to use the Tier3 cluster here at UTA much less about what it is or where its located. It is my idea that if I can provide a central place where all of this information is located, I can expidite the progress of this group, as well as the HEP (High Energy Physics) Community here at UTA. People will come and go at UTA, but here are the current students and faculty you will become aquainted with. As a User to this documentation, I ask you, please help contribute to this document so that UTA's Physics Department can grow stonger, closer, and more competative in the coming years, by helping students who are learning, just like you, understand how to become a physicist. This documentation was made with MkDocs","title":"About"},{"location":"about/#hbsm-group-at-uta","text":"","title":"HBSM Group at UTA"},{"location":"about/#the-need-for-more","text":"This documentation was created out of the necessity to get new undergraduate as well as new Graduate students ready to do research as fast as possible. At the start There was very little if any documentation on how to use critical software necessary to become a particle physicist. Such as how to use ROOT, where to find papers, where to go to learn to code in C++ and python or even how to use the Tier3 cluster here at UTA much less about what it is or where its located. It is my idea that if I can provide a central place where all of this information is located, I can expidite the progress of this group, as well as the HEP (High Energy Physics) Community here at UTA. People will come and go at UTA, but here are the current students and faculty you will become aquainted with. As a User to this documentation, I ask you, please help contribute to this document so that UTA's Physics Department can grow stonger, closer, and more competative in the coming years, by helping students who are learning, just like you, understand how to become a physicist. This documentation was made with MkDocs","title":"The Need for More"},{"location":"coding/","text":"Coding Languages and the Command Line Everyone starts out with a diffent level of experience in coding, If you take me for example I came into the Graduate program here knowing little to nothing about how to code, just basic for loops and varible declarations. But in joining the High Energy Community you will no doubt have to become a master In 2 programming languages they are C++ and Python . Where I have linked the sources to thier documentation in the links there. But before you can embark on your jouney in learning these programming languaes, you must learn How to navigate your command line terminal This is what will let you take your first steps so to say, It is what will allow you to run the C++ and Python scripts that you write, as well as copy files to and from other computers. After you feel like you can do the very basic commands out of muscle memory, such as creating files and directories, moving and copying those files you can start learning about the coding languages we use on a daily basis, Python and C++.","title":"Command Line"},{"location":"coding/#coding-languages-and-the-command-line","text":"Everyone starts out with a diffent level of experience in coding, If you take me for example I came into the Graduate program here knowing little to nothing about how to code, just basic for loops and varible declarations. But in joining the High Energy Community you will no doubt have to become a master In 2 programming languages they are C++ and Python . Where I have linked the sources to thier documentation in the links there. But before you can embark on your jouney in learning these programming languaes, you must learn How to navigate your command line terminal This is what will let you take your first steps so to say, It is what will allow you to run the C++ and Python scripts that you write, as well as copy files to and from other computers. After you feel like you can do the very basic commands out of muscle memory, such as creating files and directories, moving and copying those files you can start learning about the coding languages we use on a daily basis, Python and C++.","title":"Coding Languages and the Command Line"},{"location":"github/","text":"Getting Started with Github Git hub, is a place for you to store your code on a free online cloud server, as well as provides a way for multiple people to work on the same piece of code simultaneously, similar to a google doc if youve ever used one. Lots of groups in ATLAS use Github because they have lots of members that need to work towards completing a single large project. Lets say that there is a big project that needs to be worked on, such as creating a software that will use neural networks to distinguish between physics objects and plot thier performance. Here we can see 2 major tasks, we need someone to design these neural networks and another to create the software to plot how well they are doing. It would be very difficuly for person or a single team of people to work on both aspects, given that they are both sufficiently complicated. So it would be useful to separate the project into pieces, so that one group works on creating and optimizing the neural networks and the other focuses on creating the software to plot the performance of these networks once they are up and running. In the end though, all components of the project have to be merged together to give you a final working product that does calculation and produces plots all at once. This is where github becomes useful, as it provides the ability for multiple users to copy the initial master branch of code, make thier changes and contributions, and then push it back to the \"master branch\" to be reviewed by team mates and finally merged with the final product. Each person in a team would create a copy of the code they will work on (called a fork) and put it on thier computer. They would then work on thier dedicated aspect locally and when significant changes have been made, they \"push\" thier code back to the github website for review by thier teammates. If thier teammates like the changes/ contributions, the team lead can choose to merge the contribution to the master project called the master branch. Backups Another great aspect of Git hub is that it can work as a place to store a backup of your code, so that if a project that you are working on is stored on a cluster that goes down (becomes unusable for a period of time), which they very often do, you still have access to your code and can copy or (git clone) your work to another computer or cluster to continue working. I cannot tell you how many times either the tier3 or GPU cluster went down and I was forced to rewrite major pieces of my code or wait until the cluster went back online, which sometimes can be weeks, depending on what caused the issue. So thats it for an introduction, I think this video series is pretty good at explaining what Git and Github is and how to use it. Learn Git/Github here GitLab There also exists another more comprehensive version of GitHub called GitLab, that is used to larger collaborations and besides having a cooler logo, it has expanded functionality that comes in handy when working with larger groups of people. ATLAS and other collaborations in the LCH use GitLab instead of Git hub because of its features, while we as a smaller group at UTA would more than likely use a github page to store all our code and collaborate, it is important to know of the existance of GitLab for when you eventually have to use it as part of a collaboration wide project.","title":"Github"},{"location":"github/#getting-started-with-github","text":"Git hub, is a place for you to store your code on a free online cloud server, as well as provides a way for multiple people to work on the same piece of code simultaneously, similar to a google doc if youve ever used one. Lots of groups in ATLAS use Github because they have lots of members that need to work towards completing a single large project. Lets say that there is a big project that needs to be worked on, such as creating a software that will use neural networks to distinguish between physics objects and plot thier performance. Here we can see 2 major tasks, we need someone to design these neural networks and another to create the software to plot how well they are doing. It would be very difficuly for person or a single team of people to work on both aspects, given that they are both sufficiently complicated. So it would be useful to separate the project into pieces, so that one group works on creating and optimizing the neural networks and the other focuses on creating the software to plot the performance of these networks once they are up and running. In the end though, all components of the project have to be merged together to give you a final working product that does calculation and produces plots all at once. This is where github becomes useful, as it provides the ability for multiple users to copy the initial master branch of code, make thier changes and contributions, and then push it back to the \"master branch\" to be reviewed by team mates and finally merged with the final product. Each person in a team would create a copy of the code they will work on (called a fork) and put it on thier computer. They would then work on thier dedicated aspect locally and when significant changes have been made, they \"push\" thier code back to the github website for review by thier teammates. If thier teammates like the changes/ contributions, the team lead can choose to merge the contribution to the master project called the master branch.","title":"Getting Started with Github"},{"location":"github/#backups","text":"Another great aspect of Git hub is that it can work as a place to store a backup of your code, so that if a project that you are working on is stored on a cluster that goes down (becomes unusable for a period of time), which they very often do, you still have access to your code and can copy or (git clone) your work to another computer or cluster to continue working. I cannot tell you how many times either the tier3 or GPU cluster went down and I was forced to rewrite major pieces of my code or wait until the cluster went back online, which sometimes can be weeks, depending on what caused the issue. So thats it for an introduction, I think this video series is pretty good at explaining what Git and Github is and how to use it. Learn Git/Github here","title":"Backups"},{"location":"github/#gitlab","text":"There also exists another more comprehensive version of GitHub called GitLab, that is used to larger collaborations and besides having a cooler logo, it has expanded functionality that comes in handy when working with larger groups of people. ATLAS and other collaborations in the LCH use GitLab instead of Git hub because of its features, while we as a smaller group at UTA would more than likely use a github page to store all our code and collaborate, it is important to know of the existance of GitLab for when you eventually have to use it as part of a collaboration wide project.","title":"GitLab"},{"location":"neuralnets/","text":"Getting Started with Neural Networks As of today, the field of particle physics is moving towards the use of Neural Networks to help us as particle physicists do our job. They are quickly showing they can out perform the previous data analysis tool called the Boosted Decision Tree (BDT), When It comes to distinguishing a process that we are looking for from something that we arent, since they ofentimes look similar and can be confused. The resource that I suggest using to learn about neural networks is the series by 3 blue 1 brown, here He has created a somewhat easy to follow series of videos that give you a basic idea of how a neural networks functions, as well as gets you familiar with some of the jargon. At the time of this writing, there are 4 videos, so make sure to watch them all.","title":"Neural Networks"},{"location":"neuralnets/#getting-started-with-neural-networks","text":"As of today, the field of particle physics is moving towards the use of Neural Networks to help us as particle physicists do our job. They are quickly showing they can out perform the previous data analysis tool called the Boosted Decision Tree (BDT), When It comes to distinguishing a process that we are looking for from something that we arent, since they ofentimes look similar and can be confused. The resource that I suggest using to learn about neural networks is the series by 3 blue 1 brown, here He has created a somewhat easy to follow series of videos that give you a basic idea of how a neural networks functions, as well as gets you familiar with some of the jargon. At the time of this writing, there are 4 videos, so make sure to watch them all.","title":"Getting Started with Neural Networks"},{"location":"python/","text":"Python and C++ our Main Programming Languages There are certian programming languages that people use for differnt project types, for example, if you were to try and build a website you might want to learn to code in HTML or if you were in Data science it might be useful to learn the R programming language. It just so happens that in Particle physics, the 2 programming languages that are most useful to us are C++ and Python. C++ for its speed in large projects and compatability with ROOT (Which youll learn about later), and Python, for its ease of use, compatability with common machine learning software such as Keras and Tensorflow as well as its increaced simplicity when using ROOT. Like I said previously, Its very useful to get to a working proficiency with both languages but our group has recently trasitioned to working with Python, as it is easier to use with ROOT, and for our Neural Network projects. So where can you learn how to code in these languages? Some of the youtube series that helped me get familiar with Python are here and C++ here Dont get me wrong, these series of videos are VERY long, and will take a while to get through them, but by the end you will have a better understanding of how to write code in these languages. And please, dont feel like you have to watch these videos sequentially from start to finish, skip around and watch the ones you have questions about.","title":"Python"},{"location":"python/#python-and-c-our-main-programming-languages","text":"There are certian programming languages that people use for differnt project types, for example, if you were to try and build a website you might want to learn to code in HTML or if you were in Data science it might be useful to learn the R programming language. It just so happens that in Particle physics, the 2 programming languages that are most useful to us are C++ and Python. C++ for its speed in large projects and compatability with ROOT (Which youll learn about later), and Python, for its ease of use, compatability with common machine learning software such as Keras and Tensorflow as well as its increaced simplicity when using ROOT. Like I said previously, Its very useful to get to a working proficiency with both languages but our group has recently trasitioned to working with Python, as it is easier to use with ROOT, and for our Neural Network projects. So where can you learn how to code in these languages? Some of the youtube series that helped me get familiar with Python are here and C++ here Dont get me wrong, these series of videos are VERY long, and will take a while to get through them, but by the end you will have a better understanding of how to write code in these languages. And please, dont feel like you have to watch these videos sequentially from start to finish, skip around and watch the ones you have questions about.","title":"Python and C++ our Main Programming Languages"},{"location":"root/","text":"Everything ROOT As you have probably already realized, Doing physics has a lot of prerequisites. You have to learn a LOT of stuff before you can start doing actual research. But this tool, ROOT is the primary one that we use in the High energy physics communuty. It is a programming software that's compatible with both C++ and Python and used to Analyze data and create the plots that we use to visualize our results. It has a lot of capabilities, and using it with python makes the whole process of data analysis just a bit easier, Although ROOT is notoriously complicated and difficult to learn, My hope is to try and give you the basics so you can get on your way to performing data analysis and contribute to group progress as quickly as possible.","title":"ROOT"},{"location":"root/#everything-root","text":"As you have probably already realized, Doing physics has a lot of prerequisites. You have to learn a LOT of stuff before you can start doing actual research. But this tool, ROOT is the primary one that we use in the High energy physics communuty. It is a programming software that's compatible with both C++ and Python and used to Analyze data and create the plots that we use to visualize our results. It has a lot of capabilities, and using it with python makes the whole process of data analysis just a bit easier, Although ROOT is notoriously complicated and difficult to learn, My hope is to try and give you the basics so you can get on your way to performing data analysis and contribute to group progress as quickly as possible.","title":"Everything ROOT"},{"location":"tier3/","text":"The Tier3: The High Energy Physics (HEP) Cluster If you stay in the computing world it is more than likely youll run into a situation where youll have to use a computer cluster. These computer clusters are used when you have a piece of code that requires a lot of computing resources such as the need to run on multiple processors or requires much more RAM than one computer can provide. A lot of the time in High Energy Physics we create such pieces of code. The computing cluster that we have access to in the High Energy Physics (HEP) department is called the Tier3. It is part of 3 clusters associated with the LHC (Tier1, tier2, and Tier3), and certian universities host them, we are one of them. The tier3 is what does all the computation for analysis, the others have other dedicated processes. There are other computing clusters that CERN uses, such as the GRID and lxplus, but from day to day since the tier3 is located on campus, you will be using this most of the time. In general the way that clusters are structured is as such, The Head node This is the computer that you or any person who wants to access the cluster logs into via \"ssh\". If you have not logged into the tier3 yet, you will have to contact the cluster manager at UTA, at the time of writing this is Mark Sosebee: sosebee@exchange.uta.edu who will create an account for you and give you instructions on how to log in. Here is the basic command that you will use at your command line to login to the tier3 ssh -X <user_name>@master.tier3-ATLAS.uta.edu This Head node, is where you will be doing most of your work, and where you will submit your code to what we call a batch scheduler that will effectively run your code. The Batch schedueler is the program that distributes your job to the rest of the computers connected to the head node, called the compute nodes and tells them to run the script you are writing, there are many types of bactch scheduelers that different clusters use, some popular ones are \"Slurm\", named after the famous drink in futurama, Condor which is used by the lxplus cluster that CERN uses and PBS (Portable Batch Schedueler) which we use on the tier3. Each one of these batch scheduelers has a set of commands you can type to give you information on the current jobs that are runnning on the cluster as well as commands that will submit your code to the cluster such as \"qsub\". BUT, In order to make the compute nodes process your code, instead of typing out \"qsub script_you_want_to_run.suffix\" where suffix can be anything from .py to .cxx, you must create what we call a submission script. This is a script with a siffix of \".sub\" that lets the batch schedueler know how many processors you want to use, as well as how much RAM and compute time you need. and also contains all of the commands that you want the compute nodes to execute. There will be an example of how to write a basic submission script as well as links to the documentation on all the possible options in a section below. The Compute nodes The compute nodes are what do all of the computation in your script, and they are all connected to the head node via the batch schedueler, which as I said above determines via a submission script how many compute nodes will process your code and many other computing resources are needed. The Batch Script","title":"The Tier 3"},{"location":"tier3/#the-tier3-the-high-energy-physics-hep-cluster","text":"If you stay in the computing world it is more than likely youll run into a situation where youll have to use a computer cluster. These computer clusters are used when you have a piece of code that requires a lot of computing resources such as the need to run on multiple processors or requires much more RAM than one computer can provide. A lot of the time in High Energy Physics we create such pieces of code. The computing cluster that we have access to in the High Energy Physics (HEP) department is called the Tier3. It is part of 3 clusters associated with the LHC (Tier1, tier2, and Tier3), and certian universities host them, we are one of them. The tier3 is what does all the computation for analysis, the others have other dedicated processes. There are other computing clusters that CERN uses, such as the GRID and lxplus, but from day to day since the tier3 is located on campus, you will be using this most of the time. In general the way that clusters are structured is as such,","title":"The Tier3: The High Energy Physics (HEP) Cluster"},{"location":"tier3/#the-head-node","text":"This is the computer that you or any person who wants to access the cluster logs into via \"ssh\". If you have not logged into the tier3 yet, you will have to contact the cluster manager at UTA, at the time of writing this is Mark Sosebee: sosebee@exchange.uta.edu who will create an account for you and give you instructions on how to log in. Here is the basic command that you will use at your command line to login to the tier3 ssh -X <user_name>@master.tier3-ATLAS.uta.edu This Head node, is where you will be doing most of your work, and where you will submit your code to what we call a batch scheduler that will effectively run your code. The Batch schedueler is the program that distributes your job to the rest of the computers connected to the head node, called the compute nodes and tells them to run the script you are writing, there are many types of bactch scheduelers that different clusters use, some popular ones are \"Slurm\", named after the famous drink in futurama, Condor which is used by the lxplus cluster that CERN uses and PBS (Portable Batch Schedueler) which we use on the tier3. Each one of these batch scheduelers has a set of commands you can type to give you information on the current jobs that are runnning on the cluster as well as commands that will submit your code to the cluster such as \"qsub\". BUT, In order to make the compute nodes process your code, instead of typing out \"qsub script_you_want_to_run.suffix\" where suffix can be anything from .py to .cxx, you must create what we call a submission script. This is a script with a siffix of \".sub\" that lets the batch schedueler know how many processors you want to use, as well as how much RAM and compute time you need. and also contains all of the commands that you want the compute nodes to execute. There will be an example of how to write a basic submission script as well as links to the documentation on all the possible options in a section below.","title":"The Head node"},{"location":"tier3/#the-compute-nodes","text":"The compute nodes are what do all of the computation in your script, and they are all connected to the head node via the batch schedueler, which as I said above determines via a submission script how many compute nodes will process your code and many other computing resources are needed.","title":"The Compute nodes"},{"location":"tier3/#the-batch-script","text":"","title":"The Batch Script"}]}