{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Charged Higgs UTA Quick Start Documentation The way that this documentation is structured is in a series of links. Very many resources That I used to learn the practicle skills needed to do Particle Physics were from random internet links and not books. So very often the resource that I found most helpful, wether it be a reference document, a video series on youtube, or a tutorial outlining steps will look like this. Take the first step... It's a link to cool LO-FI music that you can listen to Take what you can from these links, and add new ones to this document if you feel youve learned more from it. Meetings Our general meetings that take place every Tuesday and Friday in CPB 128 on the UTA campus, we also have a page where we post our results for the week, that also hosts a video calling software called \"vidyo\" that we use when we are unable to meet physically here . This is hosted on a CERN meeting page called \"Indico\", that hosts a video room through the Vidyo software and allows us to have a place to post our results or weekly update powerpoints. Unfortunately you are only able to access the chatroom if you have a CERN account, and it requires a pass pin to get into the chatroom. Ask us for the password to the page, as it is unwise to post it in a public place like this. Mattermost Collaboration area We as a group, as well as a lot of other CERN affiliated groups have a place where we can all collaborate and message each other in case we need some help or need to post results, before an official meeting. The software is called the Mattermost, and unfortunately, you also need a CERN account to access it. Note that this isnt just for our group, there are hundreds of mattermost channels that you can search for and join in case you are interested in certain topics. Anything from a ROOT matter most to a machine learning mattermost, and many more are available. Contributions If you would like to contribute to either this documentation , or create working examples in code , click on the corresponding links to fork a git repo. Any help is appreciated! Works in progress All of it but specifically id like to add More about ATLAS and our charged higgs process, super symmetry and all that the ATLAS detector how to write a paper using overleaf or Latex Where to find papers Getting started with a CERN account Want to make a page like this one? It was made using a software called mkdocs, and the full documentation is here mkdocs.org .","title":"Home"},{"location":"#charged-higgs-uta","text":"","title":"Charged Higgs UTA"},{"location":"#quick-start-documentation","text":"The way that this documentation is structured is in a series of links. Very many resources That I used to learn the practicle skills needed to do Particle Physics were from random internet links and not books. So very often the resource that I found most helpful, wether it be a reference document, a video series on youtube, or a tutorial outlining steps will look like this. Take the first step... It's a link to cool LO-FI music that you can listen to Take what you can from these links, and add new ones to this document if you feel youve learned more from it.","title":"Quick Start Documentation"},{"location":"#meetings","text":"Our general meetings that take place every Tuesday and Friday in CPB 128 on the UTA campus, we also have a page where we post our results for the week, that also hosts a video calling software called \"vidyo\" that we use when we are unable to meet physically here . This is hosted on a CERN meeting page called \"Indico\", that hosts a video room through the Vidyo software and allows us to have a place to post our results or weekly update powerpoints. Unfortunately you are only able to access the chatroom if you have a CERN account, and it requires a pass pin to get into the chatroom. Ask us for the password to the page, as it is unwise to post it in a public place like this.","title":"Meetings"},{"location":"#mattermost-collaboration-area","text":"We as a group, as well as a lot of other CERN affiliated groups have a place where we can all collaborate and message each other in case we need some help or need to post results, before an official meeting. The software is called the Mattermost, and unfortunately, you also need a CERN account to access it. Note that this isnt just for our group, there are hundreds of mattermost channels that you can search for and join in case you are interested in certain topics. Anything from a ROOT matter most to a machine learning mattermost, and many more are available.","title":"Mattermost Collaboration area"},{"location":"#contributions","text":"If you would like to contribute to either this documentation , or create working examples in code , click on the corresponding links to fork a git repo. Any help is appreciated!","title":"Contributions"},{"location":"#works-in-progress","text":"All of it but specifically id like to add More about ATLAS and our charged higgs process, super symmetry and all that the ATLAS detector how to write a paper using overleaf or Latex Where to find papers Getting started with a CERN account Want to make a page like this one? It was made using a software called mkdocs, and the full documentation is here mkdocs.org .","title":"Works in progress"},{"location":"Getting_started_on_the_tier3/","text":"Getting started on the tier3 After you are able to log into the tier3 with a username and password that Mark Sosebee, our current cluster technician has provided to you. It is now time to setup your computing environment so that you can get to work, using root and any other ATLAS related tools. You want to begin by logging in, and opening up and editing a hidden file in your home directory called \".bash profile\". The \".bashrc\" and \".bash_profile\" files are scripts that are immedeately run when you log into a compute node on any computer (that is linux or unix based to my knowledge), it is the file that sets up all of your environment varaibles and all of your shortcuts also called \"aliases\". These file will not be immedeately visible when using the \"ls\" command, but will be visible using the \"ls -a\" command in the terminal. Any file with a period before its name is treated as a \"hidden file\" by bash, and will not appear with a simple \"ls\". Here are the commands that you want to type out. #This command brings you back to the home directory is youre not already in it cd emacs .bash_profile you then want to copy the following lines into your \".bash_profile\" export ATLAS_LOCAL_ROOT_BASE=/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase alias setupATLAS='source ${ATLAS_LOCAL_ROOT_BASE}/user/atlasLocalSetup.sh' setupATLAS These lines make the ATLAS software that is available on the tier3, available to you as a user. These are things like \"athena\"(Contains tools for general analysis like tau analysis tools),\"root\" (allows execution of these tools), \"rucio\" (allows you to access files that are stored on the CERN grid computing cluster, where we get our data DAODs and xAODs), \"panda\" (Allows you to submit a computing job to be executed to the CERN cluster), etc. Alternatively, If you are already familiar with how to use VS Code, Cyberduck or Filezilla, you can open the \".bash_profile\" and any other file using those programs so that it is editable using your preferred text editor, which in the end will be more efficient in my experience for working than using emacs to edit files. At this point, to activate the changes that were made to the \".bash_profile\" you will either need to run ./bash_profile, or start up a new terminal. Once the above line is done, you now have the ability to access the ATLAS analysis tools. So lets start by setting up a \"Release\", the tools that are required by almost all analysis. The basic instructions to seutp a release are here https://atlassoftwaredocs.web.cern.ch/ABtutorial/release_setup/ follow the instructions on the link unitl you get to something that looks like this, cd build/ asetup 21.2.6,AnalysisBase cmake ../source/ make and then run the lines above instead, you might have noticed that I have changed 125 to a 6, this is because our tier3 cluster does not have asetup 21.2.125 available to it, and so it is unable to setup it up. lxplus should have the cutting edge versions of asetup, but the tier3 is only updated every few years or so. Once you have changed 125 to 6, everything should run smoothly. And you should be able to sart your analysis. i.e Have ROOT run over DAOD (Derived Analysis Object Data) files At Every login From now on every time you log in, and want to do some analysis. You will need to setup asetup. You wont need to go through all the instructions that We followedc above but you will have to type in the following commands. But dont worry! Theres a shortcut to setting this up, called setting up an \"alias\". cd ../build # cd to the build directory for the Athderivation you setup using the link to the release setup on lxplus above asetup --restore source x86_64-*/setup.sh Aliases, the shortcuts of command line","title":"Getting Started"},{"location":"Getting_started_on_the_tier3/#getting-started-on-the-tier3","text":"After you are able to log into the tier3 with a username and password that Mark Sosebee, our current cluster technician has provided to you. It is now time to setup your computing environment so that you can get to work, using root and any other ATLAS related tools. You want to begin by logging in, and opening up and editing a hidden file in your home directory called \".bash profile\". The \".bashrc\" and \".bash_profile\" files are scripts that are immedeately run when you log into a compute node on any computer (that is linux or unix based to my knowledge), it is the file that sets up all of your environment varaibles and all of your shortcuts also called \"aliases\". These file will not be immedeately visible when using the \"ls\" command, but will be visible using the \"ls -a\" command in the terminal. Any file with a period before its name is treated as a \"hidden file\" by bash, and will not appear with a simple \"ls\". Here are the commands that you want to type out. #This command brings you back to the home directory is youre not already in it cd emacs .bash_profile you then want to copy the following lines into your \".bash_profile\" export ATLAS_LOCAL_ROOT_BASE=/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase alias setupATLAS='source ${ATLAS_LOCAL_ROOT_BASE}/user/atlasLocalSetup.sh' setupATLAS These lines make the ATLAS software that is available on the tier3, available to you as a user. These are things like \"athena\"(Contains tools for general analysis like tau analysis tools),\"root\" (allows execution of these tools), \"rucio\" (allows you to access files that are stored on the CERN grid computing cluster, where we get our data DAODs and xAODs), \"panda\" (Allows you to submit a computing job to be executed to the CERN cluster), etc. Alternatively, If you are already familiar with how to use VS Code, Cyberduck or Filezilla, you can open the \".bash_profile\" and any other file using those programs so that it is editable using your preferred text editor, which in the end will be more efficient in my experience for working than using emacs to edit files. At this point, to activate the changes that were made to the \".bash_profile\" you will either need to run ./bash_profile, or start up a new terminal. Once the above line is done, you now have the ability to access the ATLAS analysis tools. So lets start by setting up a \"Release\", the tools that are required by almost all analysis. The basic instructions to seutp a release are here https://atlassoftwaredocs.web.cern.ch/ABtutorial/release_setup/ follow the instructions on the link unitl you get to something that looks like this, cd build/ asetup 21.2.6,AnalysisBase cmake ../source/ make and then run the lines above instead, you might have noticed that I have changed 125 to a 6, this is because our tier3 cluster does not have asetup 21.2.125 available to it, and so it is unable to setup it up. lxplus should have the cutting edge versions of asetup, but the tier3 is only updated every few years or so. Once you have changed 125 to 6, everything should run smoothly. And you should be able to sart your analysis. i.e Have ROOT run over DAOD (Derived Analysis Object Data) files","title":"Getting started on the tier3"},{"location":"Getting_started_on_the_tier3/#at-every-login","text":"From now on every time you log in, and want to do some analysis. You will need to setup asetup. You wont need to go through all the instructions that We followedc above but you will have to type in the following commands. But dont worry! Theres a shortcut to setting this up, called setting up an \"alias\". cd ../build # cd to the build directory for the Athderivation you setup using the link to the release setup on lxplus above asetup --restore source x86_64-*/setup.sh","title":"At Every login"},{"location":"Getting_started_on_the_tier3/#aliases-the-shortcuts-of-command-line","text":"","title":"Aliases, the shortcuts of command line"},{"location":"Grid_Certificates/","text":"Grid Certificates A grid certificate is a piece of information that will allow you to access the GRID for certain tasks, like downloading files or submitting jobs to their computing resources. These Grid certificates expeire on a yearly basis and you should renew them when you get the reminder from the CERN secretariate to renew. You are allowed to keep more than one grid certificate without them interfering. Just go to this website and type in your credentials. https://ca.cern.ch/ca/ and click on \"New Grid User Certificate\", this should forward you to a page where you will be asked to log in, after which you must enter your password to either create a new grid certifiate, or renew an old one. Createing a new one, and renewing an old one basically do the same job. You should then install this certificate to your browser. For Google Chrome go to settings and then search \"Manage Certificates\" and import the Grid Certificate that should be named \"myCertificate.p12\" and import that. Once you have this certificate in your downloads folder, scp that p12 file to the cluster which you will be using to dowload files with, or submit jobs from. once scped to the cluster of your choice log into that cluster, and you should see the myCertificate.p12 file in your home directory. Type these coomands in to use the grid certificate. openssl pkcs12 -in [your-cert-file] -clcerts -nokeys -out ~/.globus/usercert.pem openssl pkcs12 -in [your-cert-file] -nocerts -out ~/.globus/userkey.pem followed by ` chmod 0600 ~/.globus/usercert.pem chmod 0400 ~/.globus/userkey.pem The same, but more detailed version of the process above is outlined here. https://www.racf.bnl.gov/docs/howto/grid/installcert Once you have run the lines of code above, you can test out if the certificate is working by typing lsetup rucio and then setting up a proxy as instructed by lsetup.","title":"Grid Certificates"},{"location":"Grid_Certificates/#grid-certificates","text":"A grid certificate is a piece of information that will allow you to access the GRID for certain tasks, like downloading files or submitting jobs to their computing resources. These Grid certificates expeire on a yearly basis and you should renew them when you get the reminder from the CERN secretariate to renew. You are allowed to keep more than one grid certificate without them interfering. Just go to this website and type in your credentials. https://ca.cern.ch/ca/ and click on \"New Grid User Certificate\", this should forward you to a page where you will be asked to log in, after which you must enter your password to either create a new grid certifiate, or renew an old one. Createing a new one, and renewing an old one basically do the same job. You should then install this certificate to your browser. For Google Chrome go to settings and then search \"Manage Certificates\" and import the Grid Certificate that should be named \"myCertificate.p12\" and import that. Once you have this certificate in your downloads folder, scp that p12 file to the cluster which you will be using to dowload files with, or submit jobs from. once scped to the cluster of your choice log into that cluster, and you should see the myCertificate.p12 file in your home directory. Type these coomands in to use the grid certificate. openssl pkcs12 -in [your-cert-file] -clcerts -nokeys -out ~/.globus/usercert.pem openssl pkcs12 -in [your-cert-file] -nocerts -out ~/.globus/userkey.pem followed by ` chmod 0600 ~/.globus/usercert.pem chmod 0400 ~/.globus/userkey.pem The same, but more detailed version of the process above is outlined here. https://www.racf.bnl.gov/docs/howto/grid/installcert Once you have run the lines of code above, you can test out if the certificate is working by typing lsetup rucio and then setting up a proxy as instructed by lsetup.","title":"Grid Certificates"},{"location":"The_LHC/","text":"The LHC (Large Hadron Collider) We get our data from the ATLAS detector at the Large hadron Collider in Geneva Switzerland. The LHC is a proton collider that is 27km (16 miles) in circumference and has components ranging from 50 to 175 meters underground. We put or collider underground so that particles coming from outer space get caught in the earth above it, and don\u2019t interfere with our experiments. It accelerates bunches of protons to a center of mass energy of 13TeV and collides them at 4 points along the ring where the detectors are located, every 25 Nano-seconds. The detectors are called LHCb, ATLAS, ALICE and CMS. Once the protons are collided, they create a shower of other particles that are then deposited into the walls of our detector, where they can then be interpreted as data for us to work with. Now its important to note that most of the particles created in the collision have a very short lifetime, and decay before they reach the walls of the detector. They decay into more stable, Final state particles, that we can then use to reconstruct what created them. The ATLAS Detector","title":"The Detectors"},{"location":"The_LHC/#the-lhc-large-hadron-collider","text":"We get our data from the ATLAS detector at the Large hadron Collider in Geneva Switzerland. The LHC is a proton collider that is 27km (16 miles) in circumference and has components ranging from 50 to 175 meters underground. We put or collider underground so that particles coming from outer space get caught in the earth above it, and don\u2019t interfere with our experiments. It accelerates bunches of protons to a center of mass energy of 13TeV and collides them at 4 points along the ring where the detectors are located, every 25 Nano-seconds. The detectors are called LHCb, ATLAS, ALICE and CMS. Once the protons are collided, they create a shower of other particles that are then deposited into the walls of our detector, where they can then be interpreted as data for us to work with. Now its important to note that most of the particles created in the collision have a very short lifetime, and decay before they reach the walls of the detector. They decay into more stable, Final state particles, that we can then use to reconstruct what created them.","title":"The LHC (Large Hadron Collider)"},{"location":"The_LHC/#the-atlas-detector","text":"","title":"The ATLAS Detector"},{"location":"Your_Text_Editor/","text":"Like a sharp knife to a hunter, your text editor will an essential part of your work flow. It will be essential that you choose one and understand its shortcuts, knowing how to use your text editor proprly will make a world of difference when it comes to getting your work done efficiently. There are many text editors to choose from, Sublime Text 3, Atom, Vim, emacs, nano, Notepad ++. My editor of choice is Sublime Text 3, I use it almost exclusively in conjunction with Cyberduck. but in situations where you arent able to setup an stfp software like CyberDuck with a certian cluster, I would suggest you beome somewhat familiar with emacs, it is installed by default on most computer clusters and","title":"Text Editors"},{"location":"about/","text":"HBSM Group at UTA The Need for More This documentation was created out of the necessity to get new undergraduate as well as new Graduate students ready to do research as fast as possible. At the start There was very little if any documentation on how to use critical software necessary to become a particle physicist. Such as how to use ROOT, where to find papers, where to go to learn to code in C++ and python or even how to use the Tier3 cluster here at UTA much less about what it is or where its located. It is my idea that if I can provide a central place where all of this information is located, I can expidite the progress of this group, as well as the HEP (High Energy Physics) Community here at UTA. People will come and go at UTA, but here are the current students and faculty you will become aquainted with. As a User to this documentation, I ask you, please help contribute to this document so that UTA's Physics Department can grow stonger, closer, and more competative in the coming years, by helping students who are learning, just like you, understand how to become a physicist. This documentation was made with MkDocs","title":"About"},{"location":"about/#hbsm-group-at-uta","text":"","title":"HBSM Group at UTA"},{"location":"about/#the-need-for-more","text":"This documentation was created out of the necessity to get new undergraduate as well as new Graduate students ready to do research as fast as possible. At the start There was very little if any documentation on how to use critical software necessary to become a particle physicist. Such as how to use ROOT, where to find papers, where to go to learn to code in C++ and python or even how to use the Tier3 cluster here at UTA much less about what it is or where its located. It is my idea that if I can provide a central place where all of this information is located, I can expidite the progress of this group, as well as the HEP (High Energy Physics) Community here at UTA. People will come and go at UTA, but here are the current students and faculty you will become aquainted with. As a User to this documentation, I ask you, please help contribute to this document so that UTA's Physics Department can grow stonger, closer, and more competative in the coming years, by helping students who are learning, just like you, understand how to become a physicist. This documentation was made with MkDocs","title":"The Need for More"},{"location":"athena_eventloop/","text":"ATLAS Releases The ATLAS experiment releases a complete set ofsoftware packages that allow you to conduct research using variaous tools that they provide, that is called a \"Release\". These Releases are preiodically updated by variaous collaborators making improvements to the packages in the software, these releases are numbered, with the latest stable release being Release 22. Before this came Release 21, which is still widely used by researchers as of May 3rd 2021, since many of the analysis tools are still valid. As time goes on, each group slowly transitions to using the latest release, as they contain the most up to date ATLAS reccomendations for particle objects as well as particle variables that should be used. Before you conduct any sort of research, you must setup one of these releases on an ATLAS enabled machine or computer cluster. lxplus or the tier2 should work. After you setup this release, you should be able to run simple scripts that will allow you to loop over the data in an AOD (Analysis Object Data) using either a pything script or a c++ script. Bwlos is a good ATLAS resource that helps you setup a release, a version of Release 21 to be specific. Setting up a Release You can ignore anything in \"Building your own release (optional & advanced)\" and after. it is unnecessary to our introduction. Athena Athena is one of the main software packages in the Release and is what allows most of the research to be done. The website below gives a good introduction to what you need to know about it. A Guide to Athena","title":"Athena and Eventloop"},{"location":"athena_eventloop/#atlas-releases","text":"The ATLAS experiment releases a complete set ofsoftware packages that allow you to conduct research using variaous tools that they provide, that is called a \"Release\". These Releases are preiodically updated by variaous collaborators making improvements to the packages in the software, these releases are numbered, with the latest stable release being Release 22. Before this came Release 21, which is still widely used by researchers as of May 3rd 2021, since many of the analysis tools are still valid. As time goes on, each group slowly transitions to using the latest release, as they contain the most up to date ATLAS reccomendations for particle objects as well as particle variables that should be used. Before you conduct any sort of research, you must setup one of these releases on an ATLAS enabled machine or computer cluster. lxplus or the tier2 should work. After you setup this release, you should be able to run simple scripts that will allow you to loop over the data in an AOD (Analysis Object Data) using either a pything script or a c++ script. Bwlos is a good ATLAS resource that helps you setup a release, a version of Release 21 to be specific. Setting up a Release You can ignore anything in \"Building your own release (optional & advanced)\" and after. it is unnecessary to our introduction.","title":"ATLAS Releases"},{"location":"athena_eventloop/#athena","text":"Athena is one of the main software packages in the Release and is what allows most of the research to be done. The website below gives a good introduction to what you need to know about it. A Guide to Athena","title":"Athena"},{"location":"coding/","text":"Coding Languages and the Command Line Everyone starts out with a diffent level of experience in coding, If you take me for example I came into the Graduate program here knowing little to nothing about how to code, just basic for loops and varible declarations. But in joining the High Energy Community you will no doubt have to become a master In 2 programming languages they are C++ and Python . Where I have linked the sources to thier documentation in the links there. But before you can embark on your jouney in learning these programming languaes, you must learn How to navigate your command line terminal This is what will let you take your first steps so to say, It is what will allow you to run the C++ and Python scripts that you write, as well as copy files to and from other computers. After you feel like you can do the very basic commands out of muscle memory, such as creating files and directories, moving and copying those files you can start learning about the coding languages we use on a daily basis, Python and C++.","title":"Command Line"},{"location":"coding/#coding-languages-and-the-command-line","text":"Everyone starts out with a diffent level of experience in coding, If you take me for example I came into the Graduate program here knowing little to nothing about how to code, just basic for loops and varible declarations. But in joining the High Energy Community you will no doubt have to become a master In 2 programming languages they are C++ and Python . Where I have linked the sources to thier documentation in the links there. But before you can embark on your jouney in learning these programming languaes, you must learn How to navigate your command line terminal This is what will let you take your first steps so to say, It is what will allow you to run the C++ and Python scripts that you write, as well as copy files to and from other computers. After you feel like you can do the very basic commands out of muscle memory, such as creating files and directories, moving and copying those files you can start learning about the coding languages we use on a daily basis, Python and C++.","title":"Coding Languages and the Command Line"},{"location":"cyberduck/","text":"Accelerating your work flow When I first started out, I was making all of my file edits through the emacs command in the terminal. And although emacs is a very powerful text editor, it can get a little bit difficult to work with when youre using it remotely, It does not support any in window mouse clicks and it occupies the entire terminal window while youre working, making you unable to view the files and directory name that youre working in. and maybe you want to use a text editor that has a lot more features, and not be tied down to using emacs, although you will definitly have to use emacs for quicker edits. It turns out there is a way that you can edit files on a remote server while using the text editor of your choice, through an sftp software called Cyberduck","title":"Cyberduck and Sublime"},{"location":"cyberduck/#accelerating-your-work-flow","text":"When I first started out, I was making all of my file edits through the emacs command in the terminal. And although emacs is a very powerful text editor, it can get a little bit difficult to work with when youre using it remotely, It does not support any in window mouse clicks and it occupies the entire terminal window while youre working, making you unable to view the files and directory name that youre working in. and maybe you want to use a text editor that has a lot more features, and not be tied down to using emacs, although you will definitly have to use emacs for quicker edits. It turns out there is a way that you can edit files on a remote server while using the text editor of your choice, through an sftp software called Cyberduck","title":"Accelerating your work flow"},{"location":"data_analysis/","text":"Data Analysis Much of the basics of a particle physics analysis can be found in this document. Chapter 3 is especially immuminating to the meaning of 5 sigma. The level of certianty that we must reach in LHC experiments to claim a discovery. Data analysis for particle physics paper When starting a project, you must first be able to access the data that you will be using, for this ATLAS uses what we call the AMI, the ATLAS Metadata Interface. This is a tool to find datasets and obtain detailed information on them.","title":"Analysis"},{"location":"data_analysis/#data-analysis","text":"Much of the basics of a particle physics analysis can be found in this document. Chapter 3 is especially immuminating to the meaning of 5 sigma. The level of certianty that we must reach in LHC experiments to claim a discovery. Data analysis for particle physics paper When starting a project, you must first be able to access the data that you will be using, for this ATLAS uses what we call the AMI, the ATLAS Metadata Interface. This is a tool to find datasets and obtain detailed information on them.","title":"Data Analysis"},{"location":"github/","text":"Getting Started with Github Git hub, is a place for you to store your code on a free online cloud server, as well as provides a way for multiple people to work on the same piece of code simultaneously, similar to a google doc if youve ever used one. Lots of groups in ATLAS use Github because they have lots of members that need to work towards completing a single large project. Lets say that there is a big project that needs to be worked on, such as creating a software that will use neural networks to distinguish between physics objects and plot thier performance. Here we can see 2 major tasks, we need someone to design these neural networks and another to create the software to plot how well they are doing. It would be very difficuly for person or a single team of people to work on both aspects, given that they are both sufficiently complicated. So it would be useful to separate the project into pieces, so that one group works on creating and optimizing the neural networks and the other focuses on creating the software to plot the performance of these networks once they are up and running. In the end though, all components of the project have to be merged together to give you a final working product that does calculation and produces plots all at once. This is where github becomes useful, as it provides the ability for multiple users to copy the initial master branch of code, make thier changes and contributions, and then push it back to the \"master branch\" to be reviewed by team mates and finally merged with the final product. Each person in a team would create a copy of the code they will work on (called a fork) and put it on thier computer. They would then work on thier dedicated aspect locally and when significant changes have been made, they \"push\" thier code back to the github website for review by thier teammates. If thier teammates like the changes/ contributions, the team lead can choose to merge the contribution to the master project called the master branch. Backups Another great aspect of Git hub is that it can work as a place to store a backup of your code, so that if a project that you are working on is stored on a cluster that goes down (becomes unusable for a period of time), which they very often do, you still have access to your code and can copy or (git clone) your work to another computer or cluster to continue working. I cannot tell you how many times either the tier3 or GPU cluster went down and I was forced to rewrite major pieces of my code or wait until the cluster went back online, which sometimes can be weeks, depending on what caused the issue. So thats it for an introduction, I think this video series is pretty good at explaining what Git and Github is and how to use it. Learn Git/Github here GitLab There also exists another more comprehensive version of GitHub called GitLab, that is used to larger collaborations and besides having a cooler logo, it has expanded functionality that comes in handy when working with larger groups of people. ATLAS and other collaborations in the LCH use GitLab instead of Git hub because of its features, while we as a smaller group at UTA would more than likely use a github page to store all our code and collaborate, it is important to know of the existance of GitLab for when you eventually have to use it as part of a collaboration wide project.","title":"Github"},{"location":"github/#getting-started-with-github","text":"Git hub, is a place for you to store your code on a free online cloud server, as well as provides a way for multiple people to work on the same piece of code simultaneously, similar to a google doc if youve ever used one. Lots of groups in ATLAS use Github because they have lots of members that need to work towards completing a single large project. Lets say that there is a big project that needs to be worked on, such as creating a software that will use neural networks to distinguish between physics objects and plot thier performance. Here we can see 2 major tasks, we need someone to design these neural networks and another to create the software to plot how well they are doing. It would be very difficuly for person or a single team of people to work on both aspects, given that they are both sufficiently complicated. So it would be useful to separate the project into pieces, so that one group works on creating and optimizing the neural networks and the other focuses on creating the software to plot the performance of these networks once they are up and running. In the end though, all components of the project have to be merged together to give you a final working product that does calculation and produces plots all at once. This is where github becomes useful, as it provides the ability for multiple users to copy the initial master branch of code, make thier changes and contributions, and then push it back to the \"master branch\" to be reviewed by team mates and finally merged with the final product. Each person in a team would create a copy of the code they will work on (called a fork) and put it on thier computer. They would then work on thier dedicated aspect locally and when significant changes have been made, they \"push\" thier code back to the github website for review by thier teammates. If thier teammates like the changes/ contributions, the team lead can choose to merge the contribution to the master project called the master branch.","title":"Getting Started with Github"},{"location":"github/#backups","text":"Another great aspect of Git hub is that it can work as a place to store a backup of your code, so that if a project that you are working on is stored on a cluster that goes down (becomes unusable for a period of time), which they very often do, you still have access to your code and can copy or (git clone) your work to another computer or cluster to continue working. I cannot tell you how many times either the tier3 or GPU cluster went down and I was forced to rewrite major pieces of my code or wait until the cluster went back online, which sometimes can be weeks, depending on what caused the issue. So thats it for an introduction, I think this video series is pretty good at explaining what Git and Github is and how to use it. Learn Git/Github here","title":"Backups"},{"location":"github/#gitlab","text":"There also exists another more comprehensive version of GitHub called GitLab, that is used to larger collaborations and besides having a cooler logo, it has expanded functionality that comes in handy when working with larger groups of people. ATLAS and other collaborations in the LCH use GitLab instead of Git hub because of its features, while we as a smaller group at UTA would more than likely use a github page to store all our code and collaborate, it is important to know of the existance of GitLab for when you eventually have to use it as part of a collaboration wide project.","title":"GitLab"},{"location":"madgraph/","text":"Getting Started with MadGraph Any particle physics analysis needs to be able to simulate the physics processes that we beleive to be happening before we look for them in the actual real life data. And there are a suite of different software that allows you to simulate these physics processes, which means creating proton proton collisions that create new particles which then decay the way that they do in real life. The first step in the simulation process is simulating these proton proton collisions to a given procss that we are interested in. We can simulate these processes using a software that we call MadGraph As stated, MadGraph simulates different particle interations that are of interest to you. It also allows you to simulate physics processes that are outside the standard model by importing the model that you are interested in. for example, Madgraph allows you to simulate ''' p p > t t~ ''' Which is a completely standard model process (proton proton goes to top and an anti-top), or if you import the MSSM model (Minimally super symmetric model) you can gain access to all the particles in that theory and generate this process ''' p p > st st~ ''' Which is proton proton goes to stau anti-stau, where the stau is a super symmetric particle instructions on how to get started are below Create an account with MadGraph using the following link MadGraph account creation Once you created your account, go to the downloads link there and log in Once logged in right click on the \"MadGraph5_aMC@NLO\" link and select \"Copy link address\" Log into the tier3 and type \"wget the_link_you_just_copied\" Untar this file by doing tar -xvf file_you_just_downloaded. then navigate to the folder that was just created and type ./bin/mg5_aMC or python2.7 ./bin/mg5_amc if this does not work at first type \"tutorial\" and the madgraph tutorial should take it from there The section below will answer some questions about whats going on in madgraph, these are some questions I had anyway. FAQ:","title":"MadGraph"},{"location":"madgraph/#getting-started-with-madgraph","text":"Any particle physics analysis needs to be able to simulate the physics processes that we beleive to be happening before we look for them in the actual real life data. And there are a suite of different software that allows you to simulate these physics processes, which means creating proton proton collisions that create new particles which then decay the way that they do in real life. The first step in the simulation process is simulating these proton proton collisions to a given procss that we are interested in. We can simulate these processes using a software that we call MadGraph As stated, MadGraph simulates different particle interations that are of interest to you. It also allows you to simulate physics processes that are outside the standard model by importing the model that you are interested in. for example, Madgraph allows you to simulate ''' p p > t t~ ''' Which is a completely standard model process (proton proton goes to top and an anti-top), or if you import the MSSM model (Minimally super symmetric model) you can gain access to all the particles in that theory and generate this process ''' p p > st st~ ''' Which is proton proton goes to stau anti-stau, where the stau is a super symmetric particle instructions on how to get started are below Create an account with MadGraph using the following link MadGraph account creation Once you created your account, go to the downloads link there and log in Once logged in right click on the \"MadGraph5_aMC@NLO\" link and select \"Copy link address\" Log into the tier3 and type \"wget the_link_you_just_copied\" Untar this file by doing tar -xvf file_you_just_downloaded. then navigate to the folder that was just created and type ./bin/mg5_aMC or python2.7 ./bin/mg5_amc if this does not work at first type \"tutorial\" and the madgraph tutorial should take it from there The section below will answer some questions about whats going on in madgraph, these are some questions I had anyway. FAQ:","title":"Getting Started with MadGraph"},{"location":"neuralnets/","text":"Getting Started with Neural Networks As of today, the field of particle physics is moving towards the use of Neural Networks to help us as particle physicists do our job. They are quickly showing they can out perform the previous data analysis tool called the Boosted Decision Tree (BDT), When It comes to distinguishing a process that we are looking for from something that we arent, since they ofentimes look similar and can be confused. The resource that I suggest using to learn about neural networks is the series by 3 blue 1 brown, here But what is a Neural Network? | Deep learning, chapter 1 He has created a somewhat easy to follow series of videos that give you a basic idea of how a neural networks functions, as well as gets you familiar with some of the jargon. At the time of this writing, there are 4 videos, so make sure to watch them all. They will serve as a sort of primer for the second document below, or vice versa. Another useful document is here Neural Networks From Scratch This was a resource suggested by Susan Bataju, a member of our group. Its a tutorial series that teaches you about 3 of the most common types of neural networks, the Deep, Recurrant and Convolutional Neural networks, (DNN,RNN and CNN. Its a pretty detailed set of tutorials that even gives some examples in code of how to build a neural net.","title":"Neural Networks"},{"location":"neuralnets/#getting-started-with-neural-networks","text":"As of today, the field of particle physics is moving towards the use of Neural Networks to help us as particle physicists do our job. They are quickly showing they can out perform the previous data analysis tool called the Boosted Decision Tree (BDT), When It comes to distinguishing a process that we are looking for from something that we arent, since they ofentimes look similar and can be confused. The resource that I suggest using to learn about neural networks is the series by 3 blue 1 brown, here But what is a Neural Network? | Deep learning, chapter 1 He has created a somewhat easy to follow series of videos that give you a basic idea of how a neural networks functions, as well as gets you familiar with some of the jargon. At the time of this writing, there are 4 videos, so make sure to watch them all. They will serve as a sort of primer for the second document below, or vice versa. Another useful document is here Neural Networks From Scratch This was a resource suggested by Susan Bataju, a member of our group. Its a tutorial series that teaches you about 3 of the most common types of neural networks, the Deep, Recurrant and Convolutional Neural networks, (DNN,RNN and CNN. Its a pretty detailed set of tutorials that even gives some examples in code of how to build a neural net.","title":"Getting Started with Neural Networks"},{"location":"overleaf/","text":"Some time along your career as a scientist, you will have to document your findings. You may have already encounted the fancy looking documents of these findings either on the arXiv where all of the prepublication physics documents are stored, or elsewhere. More than Likely, these documents were produced using a program called LateX , pronounced \"Lah-tech\", that the scientific community uses to produce professional papers. There is a website called Overleaf.com that provides an online area for you to write up these documents. You can even link it to your github account so that you may produce backups of your work and version it. There are a few ways to use LateX, you can either use overleaf, or download the latex software yourself and create your documents there. The reason for the 2 methods of use is because LateX uses computing resources to transform ordinary looking text, into the fancy stuff that you see in the papers, And so when you choose to use overleaf, youre using thier cloud computers to compile this text. If you download LateX for yourself, you can use your own computer to compile the text and make the document. This becomes useful if you have a very large document with figures and chapters that will take a while to compile, you may be better off in that case using your own computer to compile the text, instead of using Overleaf's limited resources, which are good for small to medium projects. Here is an example of a LateX document","title":"Writing Papers"},{"location":"python/","text":"Python and C++ our Main Programming Languages There are certian programming languages that people use for differnt project types, for example, if you were to try and build a website you might want to learn to code in HTML or if you were in Data science it might be useful to learn the R programming language. It just so happens that in Particle physics, the 2 programming languages that are most useful to us are C++ and Python. C++ for its speed in large projects and compatability with ROOT (Which youll learn about later), and Python, for its ease of use, compatability with common machine learning software such as Keras and Tensorflow as well as its increaced simplicity when using ROOT. Like I said previously, Its very useful to get to a working proficiency with both languages but our group has recently trasitioned to working with Python, as it is easier to use with ROOT, and for our Neural Network projects. So where can you learn how to code in these languages? Some of the youtube series that helped me get familiar with Python are here and C++ here Dont get me wrong, these series of videos are VERY long, and will take a while to get through them, but by the end you will have a better understanding of how to write code in these languages. And please, dont feel like you have to watch these videos sequentially from start to finish, skip around and watch the ones you have questions about.","title":"Python/C++"},{"location":"python/#python-and-c-our-main-programming-languages","text":"There are certian programming languages that people use for differnt project types, for example, if you were to try and build a website you might want to learn to code in HTML or if you were in Data science it might be useful to learn the R programming language. It just so happens that in Particle physics, the 2 programming languages that are most useful to us are C++ and Python. C++ for its speed in large projects and compatability with ROOT (Which youll learn about later), and Python, for its ease of use, compatability with common machine learning software such as Keras and Tensorflow as well as its increaced simplicity when using ROOT. Like I said previously, Its very useful to get to a working proficiency with both languages but our group has recently trasitioned to working with Python, as it is easier to use with ROOT, and for our Neural Network projects. So where can you learn how to code in these languages? Some of the youtube series that helped me get familiar with Python are here and C++ here Dont get me wrong, these series of videos are VERY long, and will take a while to get through them, but by the end you will have a better understanding of how to write code in these languages. And please, dont feel like you have to watch these videos sequentially from start to finish, skip around and watch the ones you have questions about.","title":"Python and C++ our Main Programming Languages"},{"location":"roostats/","text":"ROOStats","title":"Roostats"},{"location":"roostats/#roostats","text":"","title":"ROOStats"},{"location":"root/","text":"Everything ROOT As you have probably already realized, Doing physics has a lot of prerequisites. You have to learn a LOT of stuff before you can start doing actual research. But this tool, ROOT is the primary one that we use in the High energy physics communuty. It is a programming software that's compatible with both C++ and Python and used to Analyze data and create the plots that we use to visualize our results. It has a lot of capabilities, and using it with python makes the whole process of data analysis just a bit easier, Although ROOT is notoriously complicated and difficult to learn, My hope is to try and give you the basics so you can get on your way to performing data analysis and contribute to group progress as quickly as possible.","title":"ROOT"},{"location":"root/#everything-root","text":"As you have probably already realized, Doing physics has a lot of prerequisites. You have to learn a LOT of stuff before you can start doing actual research. But this tool, ROOT is the primary one that we use in the High energy physics communuty. It is a programming software that's compatible with both C++ and Python and used to Analyze data and create the plots that we use to visualize our results. It has a lot of capabilities, and using it with python makes the whole process of data analysis just a bit easier, Although ROOT is notoriously complicated and difficult to learn, My hope is to try and give you the basics so you can get on your way to performing data analysis and contribute to group progress as quickly as possible.","title":"Everything ROOT"},{"location":"supercomputer/","text":"The GPU Cluster There exists another cluster on campus that is different from the tier3. It is a GPU super computer managed by Dr.Farbin here at UTA. This cluster is similar to the tier3, but instead of having compute nodes that have many CPUs the nodes on the super computer host multiple GPUs (Graphical Processing units) that are capable of executing many tasks in parallel. The general purpose for a GPU cluster is to do work in training machine learning algorithms such as the ones found in Neural networks. With Dr. Farbin's permission, and if you begin to do work with neural networks, you will find this cluster to be extremely useful. You will need to email Dr.Farbin at afarbin@uta.edu for him to give you an account on the supercomputer, but once you have all the login information you should be able to access the computer by ssh-ing into it with the command below ssh -X <user_name>@orodruin.uta.edu The Head node The head node, just like the head node on the tier3, is where you will be doing most of your work. Similar to ther tier3, the head node on the super computer does have some GPU resources, it has 4 GeForce GTX 1080 GPUs that can be used to test short scripts, although it is discouraged to run very large and computing intensive pieces of code on it, since it will interfere with others trying to login in or test thier scripts. It is best practice to submit your code via the the PBS batch schedueler like on the tier3 or log into the compute node directly and submit your code ro be run. GPU compute nodes The super computer has 4 GPU compute nodes and 1 ordinary CPU node, which can be accessed once you have logged into the main node as indicated above. Thier names are The Count, which has 10 TITAN X (Pascal) GPUs for use ssh thecount Thing One, has 4 GeForce GTX 1080 GPUs and together are just as powerful as the 10 GPUs on the count. ssh thingone Thing Two, which also has 4 GeForce GTX 1080 GPUs ssh thingtwo","title":"The Super Computer"},{"location":"supercomputer/#the-gpu-cluster","text":"There exists another cluster on campus that is different from the tier3. It is a GPU super computer managed by Dr.Farbin here at UTA. This cluster is similar to the tier3, but instead of having compute nodes that have many CPUs the nodes on the super computer host multiple GPUs (Graphical Processing units) that are capable of executing many tasks in parallel. The general purpose for a GPU cluster is to do work in training machine learning algorithms such as the ones found in Neural networks. With Dr. Farbin's permission, and if you begin to do work with neural networks, you will find this cluster to be extremely useful. You will need to email Dr.Farbin at afarbin@uta.edu for him to give you an account on the supercomputer, but once you have all the login information you should be able to access the computer by ssh-ing into it with the command below ssh -X <user_name>@orodruin.uta.edu","title":"The GPU Cluster"},{"location":"supercomputer/#the-head-node","text":"The head node, just like the head node on the tier3, is where you will be doing most of your work. Similar to ther tier3, the head node on the super computer does have some GPU resources, it has 4 GeForce GTX 1080 GPUs that can be used to test short scripts, although it is discouraged to run very large and computing intensive pieces of code on it, since it will interfere with others trying to login in or test thier scripts. It is best practice to submit your code via the the PBS batch schedueler like on the tier3 or log into the compute node directly and submit your code ro be run.","title":"The Head node"},{"location":"supercomputer/#gpu-compute-nodes","text":"The super computer has 4 GPU compute nodes and 1 ordinary CPU node, which can be accessed once you have logged into the main node as indicated above. Thier names are The Count, which has 10 TITAN X (Pascal) GPUs for use ssh thecount Thing One, has 4 GeForce GTX 1080 GPUs and together are just as powerful as the 10 GPUs on the count. ssh thingone Thing Two, which also has 4 GeForce GTX 1080 GPUs ssh thingtwo","title":"GPU compute nodes"},{"location":"tier3/","text":"Batch Scripting on the Tier3: The High Energy Physics (HEP) Cluster If you stay in the computing world it is more than likely youll run into a situation where youll have to use a computer cluster. These computer clusters are used when you have a piece of code that requires a lot of computing resources such as the need to run on multiple processors or requires much more RAM than one computer can provide. A lot of the time in High Energy Physics we create such pieces of code. The computing cluster that we have access to in the High Energy Physics (HEP) department is called the Tier3. It is part of 3 clusters associated with the LHC (Tier1, tier2, and Tier3), and certian universities host them, we are one of them. The tier3 is what does all the computation for analysis, the others have other dedicated processes. There are other computing clusters that CERN uses, such as the GRID and lxplus, but from day to day since the tier3 is located on campus, you will be using this most of the time. In general the way that clusters are structured is as such, The Head node This is the computer that you or any person who wants to access the cluster logs into via \"ssh\". If you have not logged into the tier3 yet, you will have to contact the cluster manager at UTA, at the time of writing this is Mark Sosebee: sosebee@exchange.uta.edu who will create an account for you and give you instructions on how to log in. Here is the basic command that you will use at your command line to login to the tier3 ssh -X <user_name>@master.tier3-ATLAS.uta.edu This Head node, is where you will be doing most of your work, and where you will submit your code to what we call a batch scheduler that will effectively run your code. The Batch schedueler is the program that distributes your job to the rest of the computers connected to the head node, called the compute nodes and tells them to run the script you are writing, there are many types of bactch scheduelers that different clusters use, some popular ones are \"Slurm\", named after the famous drink in futurama, Condor which is used by the lxplus cluster that CERN uses and PBS (Portable Batch Schedueler) which we use on the tier3. Each one of these batch scheduelers has a set of commands you can type to give you information on the current jobs that are runnning on the cluster as well as commands that will submit your code to the cluster such as \"qsub\". BUT, In order to make the compute nodes process your code, instead of typing out qsub script_you_want_to_run.suffix where suffix can be anything from .py to .cxx, you must create what we call a submission script. This is a script with a siffix of \".sub\" that lets the batch schedueler know how many processors you want to use, as well as how much RAM and compute time you need. and also contains all of the commands that you want the compute nodes to execute. There will be an example of how to write a basic submission script as well as links to the documentation on all the possible options in a section below. From here it is important to know, that any code submittted to the cluster via a batch script is called a Job As of now, the tier-3 cluster is contained in one rack in CPB 115. This is the machine room where the tier-2 system we operate for ATLAS is located. By comparison the tier-2 system occupies 27 racks. The Compute nodes The compute nodes are what do all of the computation in your script, and they are all connected to the head node via the batch schedueler, which as I said above determines via a submission script how many compute nodes will process your code and many other computing resources are needed. The Batch Script We create batch scripts so that we can submit our code to the cluster, this is to allow the head node of the tier3 to run as smoothly as possible with no connection lag to any user on the cluster. Once the script is made, you would submit it to the cluster by typing qsub name_of_batch_script.pbs So lets learn how to make this script, using that basic histogram making file we worked on. A very basic example looks like this #~!/bin/bash #PBS -l qos=hep #PBS -M juan.cardenas@mavs.uta.edu #PBS -m abe #PBS -e /cluster/home/jcardenas34/qualification_task/HDF_dir/ #PBS -o /cluster/home/jcardenas34/qualification_task/HDF_dir #PBS -j oe #PBS -l walltime=72:00:00 athDerivation cd /cluster/home/jcardenas34/new_group_member_introduction/ python Intro_root_histogram_maker.py So what do the #PBS lines and the rest mean? #~!/bin/bash #PBS -M juan.cardenas@mavs.uta.edu #PBS -m abe #PBS -e /cluster/home/jcardenas34/qualification_task/ #PBS -o /cluster/home/jcardenas34/qualification_task/ #PBS -j oe #PBS -l walltime=72:00:00 #PBS -l qos=hep These lines that start with #PBS tell the batch scheuduler certian directions on what to do and so they are called directives. Lets go line by line. ~!/bin/bash Says that we want to run this script in the terminal PBS -M juan.cardenas@mavs.uta.edu Allows you to be notified via email when your job is completed PBS -m abe Sets the conditions under which an email should be sent, here abe stand for send an email when the job is (borted,begins,terminates) PBS -e /cluster/home/jcardenas34/qualification_task/ PBS creates an error and an output log file where the output and error messages given by your job are stored, name a path if you want to send this error to a specific place, otherwise it is created after the job is done, in the directory you submitted the job in. PBS -o /cluster/home/jcardenas34/qualification_task/ Same as above, where but where the output file is sent PBS -j oe Very important , each time a job is completed, 2 log files are made, if a lot of job files are submitted, youll get a lot of log files, and it will quickly become a lot to sort through. so the -j option joins the error and output files together. You must also specify that you want to merge them by typing o and e together after -j PBS -l walltime=72:00:00 Also extremely important , if not the most important, this is the estimated time you expect your job to finish in The format is in hrs:min:secs and if your job exceeds the amount of time you entered, your job is immedeately terminated. There is a current maximum time allowed for a job to run on each queue, on the \"hep\" queue this is 96:00:00 and you can check the rest using qstat -q PBS -l qos=hep Tells the batch schedueler which queue you want to run on, the main reason for this is to allow more or less time for your job to run. Now for what you actually want to run, your code athDerivation cd /cluster/home/jcardenas34/new_group_member_introduction/ python Intro_root_histogram_maker.py Commands that you want to execute on the compute nodes go right after all the #PBS directive lines. Anything after the #PBS lines will run as if you are running it on a regular terminal, so put any lines here that you would need to type in order to run your script. Youll notice that \"athDerivation\" is there since it is necessary to run this particular script. But any lines you plan to run should be here, from lines that you can type in the termainal like \"cd\" and \"touch\" or running code thats already in a script with python or another language. These instrucitons imply that you have created a new file \"qsub\" that you will call, \"name_of_script_you_want_to_submit_to_cluster.qsub\", by using the \"touch\" command in the terminal section, and added similar lines to the ones above with your text editor. The Queue Lets say that youve submitted a job to the check its running status using \"qstat\", this is a command for the batch schedueler that lets you view all of the jobs currently running on the cluster. Typing \"qstat\" should either show nothing, because noone is running anything on the cluster, or it should look something like this. Job id Name User Time Use S Queue ------------------------- ---------------- --------------- -------- - ----- 1441291.master ...1p1n_Test.sub jcardenas34 01:16:05 R hep You can add modifiers to the qstat command to let it give you more information, like which node your code was sent to, and how long its been running using what we call flag modifiers at the end of qstat like so, there are many others in the reference documents below # This shows a rather lengthy but full description of all jobs that have been sent to the cluster qstat -f You can also remove a job from the queue if you made a mistake by typing qdel and the jobid of the job you want to remove, for example, lets say I wanted to remove the job above from the queue, id do qdel 1441291 There are many more possible commands in teh documentation below. Reference Documents There exists user documentation and a reference guide on how to use the pbs batch system, linked below User Guide for PBS Reference guide for PBS The user guide is a more comprehensive document that teaches you about most of the aspects of the PBS (Portable batch system), the reference guide is more of a summary of all possible commands that you can input, both in the PBS script and in the command prompt How to make a Tier3 not timeout So at the time of writing, the tier3 has an annoying feature where it disconnects your login session in the terminal after a few minutes of inactivity. This is particularly annoying when you have to ssh into multiple clusters, and must leave the tier3 terminal inactive for a while, but will return to it later. When you return, you see that the termial is now inactive, and you must log in again, and navigate to where you were previously. Luckily, there are a few changes you can make in your ssh_config script that will help you extend the time before timeout. These instructions work on a mac , but if you have some sort of linux based command line terminal installed on your computer, is should work just the same. For windows you can either use \"commander\" or \"git terminal\" or \"ubuntu bash shell\", I will add more on how to install this in the Accelerating your workflow part of the page later. When you first open your terminal, you will be logged into the home directory of your personal computer. You will want to open this file with whatever text editor you prefer. /etc/ssh/ssh_config by typing, either, emacs, sublime, nano, vim, atom or your preferred text editor name, and then the line above. This is a file that tells the command prompt how to insteract with any computer that you choose to ssh into, weather that be the tier3 or another cluster somewhere else. All of the lines in this file should be commented out, except for a small section near the bottom of the page. which should look like this. Host * SendEnv LANG LC_* you want to add a few lines to it so that It looks like this Host * SendEnv LANG LC_* ServerAliveInterval 120 ServerAliveCountMax 720 and save the changes. The firstaddition we made makes it so a blank message is sent to the server every 120 seconds in order to keep the connection, the second line we added makes it so that it will log you off if this is done 720 times, so 720 x 120 seconds = 24 hours, you can adjust this second number to 30 for one hour tuntil timeout, or to 60 for 2 hours of sustained login during 2 hours of inactivity.","title":"The Tier 3"},{"location":"tier3/#batch-scripting-on-the-tier3-the-high-energy-physics-hep-cluster","text":"If you stay in the computing world it is more than likely youll run into a situation where youll have to use a computer cluster. These computer clusters are used when you have a piece of code that requires a lot of computing resources such as the need to run on multiple processors or requires much more RAM than one computer can provide. A lot of the time in High Energy Physics we create such pieces of code. The computing cluster that we have access to in the High Energy Physics (HEP) department is called the Tier3. It is part of 3 clusters associated with the LHC (Tier1, tier2, and Tier3), and certian universities host them, we are one of them. The tier3 is what does all the computation for analysis, the others have other dedicated processes. There are other computing clusters that CERN uses, such as the GRID and lxplus, but from day to day since the tier3 is located on campus, you will be using this most of the time. In general the way that clusters are structured is as such,","title":"Batch Scripting on the Tier3: The High Energy Physics (HEP) Cluster"},{"location":"tier3/#the-head-node","text":"This is the computer that you or any person who wants to access the cluster logs into via \"ssh\". If you have not logged into the tier3 yet, you will have to contact the cluster manager at UTA, at the time of writing this is Mark Sosebee: sosebee@exchange.uta.edu who will create an account for you and give you instructions on how to log in. Here is the basic command that you will use at your command line to login to the tier3 ssh -X <user_name>@master.tier3-ATLAS.uta.edu This Head node, is where you will be doing most of your work, and where you will submit your code to what we call a batch scheduler that will effectively run your code. The Batch schedueler is the program that distributes your job to the rest of the computers connected to the head node, called the compute nodes and tells them to run the script you are writing, there are many types of bactch scheduelers that different clusters use, some popular ones are \"Slurm\", named after the famous drink in futurama, Condor which is used by the lxplus cluster that CERN uses and PBS (Portable Batch Schedueler) which we use on the tier3. Each one of these batch scheduelers has a set of commands you can type to give you information on the current jobs that are runnning on the cluster as well as commands that will submit your code to the cluster such as \"qsub\". BUT, In order to make the compute nodes process your code, instead of typing out qsub script_you_want_to_run.suffix where suffix can be anything from .py to .cxx, you must create what we call a submission script. This is a script with a siffix of \".sub\" that lets the batch schedueler know how many processors you want to use, as well as how much RAM and compute time you need. and also contains all of the commands that you want the compute nodes to execute. There will be an example of how to write a basic submission script as well as links to the documentation on all the possible options in a section below. From here it is important to know, that any code submittted to the cluster via a batch script is called a Job As of now, the tier-3 cluster is contained in one rack in CPB 115. This is the machine room where the tier-2 system we operate for ATLAS is located. By comparison the tier-2 system occupies 27 racks.","title":"The Head node"},{"location":"tier3/#the-compute-nodes","text":"The compute nodes are what do all of the computation in your script, and they are all connected to the head node via the batch schedueler, which as I said above determines via a submission script how many compute nodes will process your code and many other computing resources are needed.","title":"The Compute nodes"},{"location":"tier3/#the-batch-script","text":"We create batch scripts so that we can submit our code to the cluster, this is to allow the head node of the tier3 to run as smoothly as possible with no connection lag to any user on the cluster. Once the script is made, you would submit it to the cluster by typing qsub name_of_batch_script.pbs So lets learn how to make this script, using that basic histogram making file we worked on. A very basic example looks like this #~!/bin/bash #PBS -l qos=hep #PBS -M juan.cardenas@mavs.uta.edu #PBS -m abe #PBS -e /cluster/home/jcardenas34/qualification_task/HDF_dir/ #PBS -o /cluster/home/jcardenas34/qualification_task/HDF_dir #PBS -j oe #PBS -l walltime=72:00:00 athDerivation cd /cluster/home/jcardenas34/new_group_member_introduction/ python Intro_root_histogram_maker.py So what do the #PBS lines and the rest mean? #~!/bin/bash #PBS -M juan.cardenas@mavs.uta.edu #PBS -m abe #PBS -e /cluster/home/jcardenas34/qualification_task/ #PBS -o /cluster/home/jcardenas34/qualification_task/ #PBS -j oe #PBS -l walltime=72:00:00 #PBS -l qos=hep These lines that start with #PBS tell the batch scheuduler certian directions on what to do and so they are called directives. Lets go line by line. ~!/bin/bash Says that we want to run this script in the terminal PBS -M juan.cardenas@mavs.uta.edu Allows you to be notified via email when your job is completed PBS -m abe Sets the conditions under which an email should be sent, here abe stand for send an email when the job is (borted,begins,terminates) PBS -e /cluster/home/jcardenas34/qualification_task/ PBS creates an error and an output log file where the output and error messages given by your job are stored, name a path if you want to send this error to a specific place, otherwise it is created after the job is done, in the directory you submitted the job in. PBS -o /cluster/home/jcardenas34/qualification_task/ Same as above, where but where the output file is sent PBS -j oe Very important , each time a job is completed, 2 log files are made, if a lot of job files are submitted, youll get a lot of log files, and it will quickly become a lot to sort through. so the -j option joins the error and output files together. You must also specify that you want to merge them by typing o and e together after -j PBS -l walltime=72:00:00 Also extremely important , if not the most important, this is the estimated time you expect your job to finish in The format is in hrs:min:secs and if your job exceeds the amount of time you entered, your job is immedeately terminated. There is a current maximum time allowed for a job to run on each queue, on the \"hep\" queue this is 96:00:00 and you can check the rest using qstat -q PBS -l qos=hep Tells the batch schedueler which queue you want to run on, the main reason for this is to allow more or less time for your job to run. Now for what you actually want to run, your code athDerivation cd /cluster/home/jcardenas34/new_group_member_introduction/ python Intro_root_histogram_maker.py Commands that you want to execute on the compute nodes go right after all the #PBS directive lines. Anything after the #PBS lines will run as if you are running it on a regular terminal, so put any lines here that you would need to type in order to run your script. Youll notice that \"athDerivation\" is there since it is necessary to run this particular script. But any lines you plan to run should be here, from lines that you can type in the termainal like \"cd\" and \"touch\" or running code thats already in a script with python or another language. These instrucitons imply that you have created a new file \"qsub\" that you will call, \"name_of_script_you_want_to_submit_to_cluster.qsub\", by using the \"touch\" command in the terminal section, and added similar lines to the ones above with your text editor.","title":"The Batch Script"},{"location":"tier3/#the-queue","text":"Lets say that youve submitted a job to the check its running status using \"qstat\", this is a command for the batch schedueler that lets you view all of the jobs currently running on the cluster. Typing \"qstat\" should either show nothing, because noone is running anything on the cluster, or it should look something like this. Job id Name User Time Use S Queue ------------------------- ---------------- --------------- -------- - ----- 1441291.master ...1p1n_Test.sub jcardenas34 01:16:05 R hep You can add modifiers to the qstat command to let it give you more information, like which node your code was sent to, and how long its been running using what we call flag modifiers at the end of qstat like so, there are many others in the reference documents below # This shows a rather lengthy but full description of all jobs that have been sent to the cluster qstat -f You can also remove a job from the queue if you made a mistake by typing qdel and the jobid of the job you want to remove, for example, lets say I wanted to remove the job above from the queue, id do qdel 1441291 There are many more possible commands in teh documentation below.","title":"The Queue"},{"location":"tier3/#reference-documents","text":"There exists user documentation and a reference guide on how to use the pbs batch system, linked below User Guide for PBS Reference guide for PBS The user guide is a more comprehensive document that teaches you about most of the aspects of the PBS (Portable batch system), the reference guide is more of a summary of all possible commands that you can input, both in the PBS script and in the command prompt","title":"Reference Documents"},{"location":"tier3/#how-to-make-a-tier3-not-timeout","text":"So at the time of writing, the tier3 has an annoying feature where it disconnects your login session in the terminal after a few minutes of inactivity. This is particularly annoying when you have to ssh into multiple clusters, and must leave the tier3 terminal inactive for a while, but will return to it later. When you return, you see that the termial is now inactive, and you must log in again, and navigate to where you were previously. Luckily, there are a few changes you can make in your ssh_config script that will help you extend the time before timeout. These instructions work on a mac , but if you have some sort of linux based command line terminal installed on your computer, is should work just the same. For windows you can either use \"commander\" or \"git terminal\" or \"ubuntu bash shell\", I will add more on how to install this in the Accelerating your workflow part of the page later. When you first open your terminal, you will be logged into the home directory of your personal computer. You will want to open this file with whatever text editor you prefer. /etc/ssh/ssh_config by typing, either, emacs, sublime, nano, vim, atom or your preferred text editor name, and then the line above. This is a file that tells the command prompt how to insteract with any computer that you choose to ssh into, weather that be the tier3 or another cluster somewhere else. All of the lines in this file should be commented out, except for a small section near the bottom of the page. which should look like this. Host * SendEnv LANG LC_* you want to add a few lines to it so that It looks like this Host * SendEnv LANG LC_* ServerAliveInterval 120 ServerAliveCountMax 720 and save the changes. The firstaddition we made makes it so a blank message is sent to the server every 120 seconds in order to keep the connection, the second line we added makes it so that it will log you off if this is done 720 times, so 720 x 120 seconds = 24 hours, you can adjust this second number to 30 for one hour tuntil timeout, or to 60 for 2 hours of sustained login during 2 hours of inactivity.","title":"How to make a Tier3 not timeout"}]}